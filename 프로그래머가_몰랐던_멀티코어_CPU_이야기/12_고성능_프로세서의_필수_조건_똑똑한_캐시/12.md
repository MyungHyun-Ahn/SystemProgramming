# Story 12 고성능 프로세서의 필수 조건 : 똑똑한 캐시
현대 범용 목적 마이크로프로세서의 회로 사진을 보면 절반 이상의 공간을 차지하는 부품이 CPU 캐시
* 파이프라인과 함께 프로세서의 성능을 높인 일등 공신

캐시가 필요한 이유
* 주 메모리인 DRAM에서 데이터를 가져오는 시간이 상당히 오래 걸리기 때문
* 자주 쓰고 인접한 내용을 고속의 캐시에 저장해서 명령어 완료 시간을 크게 단축

인텔 Nehalem 프로세서의 칩 모습
* 아래 있는 것이 L3 캐시, 각 코어의 절반 정도가 L1, L2 캐시에 해당
* 가장 자리에는 메모리 및 입출력 컨트롤러가 있다.

![12-01](https://github.com/MyungHyun-Ahn/SystemProgramming/assets/78206106/4480c500-c3c4-4dc7-afd4-23f251fecb8a)


## 왜 캐시가 필요하고 잘 작동할 수 있을까?
CPU 캐시가 필요한 이유
* 메인 메모리와 프로세서의 시간적, 공간적 거리가 멀어 데이터를 주고받는데 시간이 오래 걸리기 때문
* 그래서 자주 쓰고 인접한 데이터를 캐시에 보관해서 쓰는 것

일반적인 프로세서가 DRAM에서 자료를 요청해 받아오는데 보통 100사이클 이상
* 그런데 1~2사이클이면 레지스터 파일에서 데이터를 가져올 수 있다.
* 백 단위 사이클은 심각한 손실
* 프로세서와 메인 메모리의 클록 속도 차이도 문제
  * 2009년의 프로세서는 2~3GHz, 메모리는 800MHz
* 캐시가 절실한 이유 - 병목 지점이 DRAM, 디스크

온 칩, 오프 칩
* 현대의 CPU 캐시 : 온 칩(on-chip) 캐시, 칩 내부에 있는 것
* 과거의 CPU 캐시 : 오프 칩(off-chip) 캐시, 칩 외부에 있는 것

캐시는 CPU에만 있는 것이 아니다.
* 메모리 계층에서 속도, 공간, 가격 차이가 있는 두 계층 사이라면 캐시는 있을 수 있다.
  * 하드디스크의 캐시
  * DRAM 내의 행 버퍼(row buffer)
  * 소프트웨어에도 존재
  * 운영체제의 파일 시스템에도 여러 버퍼와 캐시
  * 웹 브라우저의 캐시

캐시가 잘 작동할 수 있는 원리 - 지역성(locality)
* 시간적 지역성(temporal locality)
  * 지금 어떤 데이터를 사용했다면 가까운 미래에 다시 사용할 확률이 높다는 것을 가리킴
  * 최근에 사용한 데이터를 보관, 가까운 미래에 이 데이터가 다시 사용되면 빠르게 줄 수 있다.
* 공간적 지역성(spatial locality)
  * 어떤 데이터가 사용된다면 그와 인접한 데이터에 접근할 것 같음을 뜻함
  * 데이터를 캐시에 넣을 때 그 데이터만 넣은 것이 아닌 그와 인접한 데이터도 함께 가져온다.
* 대부분의 데이터 접근이 랜덤하지 않고 지역성이라는 규칙을 띠어서 캐시가 잘 작동할 수 있다.

간단한 for 루프에서 시간적/공간적 지역성을 볼 수 있다.
~~~C++
1:  for (int i = 1; i < N; ++i)
2:    data[i] = data[i - 1] + 1;
~~~
* 시간적 지역성
  * 첫 번째 순환에서 값이 갱신되면 그 다음 순환에서 피연산자로 읽힌다.
* 공간적 지역성
  * data[1] 부터 data[N - 1]까지 접근

CPU 캐시는 데이터 접근의 두 지역성을 효과적으로 이용한다.

## 일반적인 캐시 구조
일반적인 소프트웨어에서 캐시
* 캐시는 접근 속도, 대역폭 단위 용량 당 가격 등 성능 차이가 뚜렷한 두 계층 사이에서 지역성을 활용해 자주 쓰이거나 인접한 자료를 잠시 저장한다.

캐시 모듈을 만들 때 필요한 인터페이스
1. 캐시에서 원하는 데이터를 찾는 함수
2. 새로운 데이터를 추가하는 함수
3. 이전 데이터 중 일부를 교체할 수 있는 정책이나 알고리즘
   * 캐시는 대부분 공간이 제한적이므로 캐시가 가득 찬 상태에서 새로운 데이터를 넣을 때

캐시에서 사용하는 용어
* 캐시 히트(cache hit) : 캐시에 원하는 데이터가 있을 때
* 캐시 미스(cache miss) : 캐시에서 원하는 데이터를 찾지 못했을 때
* 미스 패널티(miss penalty) : 캐시 미스가 발생하고 직접 해당 데이터를 찾아와 넣어야 하는 비용
* 히트 레이턴시(hit latency) : 캐시가 적중, 원하는 데이터를 가져오는데 드는 비용

매우 일반적인 소프트웨어 기반 캐시 시스템 구조를 그린 것

![12-02](https://github.com/MyungHyun-Ahn/SystemProgramming/assets/78206106/b6097ecc-2aab-4966-965c-9d1b42826ac7)

캐시는 여러 캐시 엔트리로 구성, 각 엔트리에는 실제 캐시하는 데이터와 부가 정보가 있다.
* 부가 정보는 일반적으로 다음으로 이루어진다.
  1. 실제 저장되는 데이터
  2. 태그(tag, 인식표)
  3. 캐시 찾기에 관련된 정보(포인터 같은 것)
  4. 캐시 교체에 관련된 정보(예, 언제 얼마나 이용되었는가)
  5. 기타 정보

주어진 데이터를 캐시에서 찾는 작업은 일반적으로 해시 테이블(hash table) 자료구조로 구현
* 캐시가 저장할 수 있는 캐시 엔트리의 개수는 가능한 전체 데이터 개수보다 보통 훨씬 작기 때문에
* 데이터를 받으면 해시 값을 구한 뒤, 해당 위치로 가서 탐색한다.

캐시 교체 알고리즘이 필요
* 오랫동안 쓰이지 않은 것을 삭제
* 교체 정책에 따라 자료구조가 다를 수 있다.

캐시를 설계
* 캐시 엔트리 구조를 정한다.
* 최적의 탐색 방법, 교체 정책을 정의한다.

CPU 캐시, 혹은 하드웨어 캐시도 기본적인 개념은 동일
* CPU 캐시는 일반적으로 SRAM(Static Random Access Memory)로 만들어진다.
  * DRAM에 비해 제조 비용이 비싸서 최대한 공간을 아껴야만 한다.
  * 해시 테이블처럼 복잡한 자료구조의 사용도 힘들고
  * 복잡한 해시 함수도 쓰기 어렵다.
* L3 캐시는 보통 eDRAM(embedded DRAM)을 이용하여 만든다.
  * SRAM 보다 레이턴시는 다소 느리지만 더 많은 용량을 구현할 수 있다.
* 캐시 교체에 필요한 부가 정보도 그 양을 줄여 비트 단위로 관리하고, 교체 알고리즘도 최대한 간단히 만들어야 한다.
  * 그렇지 않으면 캐시 레이턴시가 지나치게 길어진다.

## CPU 캐시의 기본적인 설계
CPU 캐시
* 메인 메모리와 레지스터 파일 사이의 큰 속도 격차를 줄이려고 프로세서 내에 있는 작지만 매우 빠른 기억 장치
* 데이터 접근의 시간적/공간적 지역성을 활용
* 캐시로 데이터를 가져오는데 필요한 메모리 레이턴시를 줄일 수 있다.

프로세서는 추상적으로 메모리 시스템이라는 장치에서 명령어와 데이터를 읽거나 쓴다.
* 캐시는 프로그래머의 입장에서는 있는지 없는지 모르게 처리, 특별한 최적화가 아닌 이상 고려할 필요가 없다.

메모리 시스템에는 레지스터와 DRAM이 있는데 이 사이에 캐시가 있어 큰 레이턴시 차이를 극복한다.

그런데 요즘 프로세서는 캐시가 여러 계층으로 있다.
* L1, L2 캐시 : 현대 프로세서 구조에서는 일반적
* L3 캐시 : 까지 있는 CPU도 쉽게 볼 수 있다.

보통 마지막 레벨의 칩 내 캐시를 특별히 LLC(Last Level Cache)라 한다.
* LLC 이후는 시간이 매우 오래 걸리는 칩 밖의 메모리 계층으로 이동해야 하기 때문에 특별히 구분

여러 계층의 캐시를 두는 이유
* 캐시 접근 속도와 용량 사이의 트레이드 오프가 있어 여러 단계를 두면 성능이 더 좋아지기 때문

프로세서가 메모리 시스템에 데이터를 요청하는 과정
* L1 캐시 부터 찾는다.
* 없다면 L2 캐시, 쭉쭉 내려가서 LLC 까지 찾는다.
* LLC에도 없다면 비로소 메인 메모리로 가서 데이터를 가져온다.
  * 이 데이터는 캐시에 보관되고 최종으로 프로세서에게 반환된다.

일반적으로 L1 캐시는 명령어와 데이터를 분리해 저장하는 것이 더 효과적이다.
* 많은 프로세서들이 이런 방식을 택한다.
* L2 캐시 이상은 분리하지 않는다.

캐시 설계는 다음 네 가지 문제를 풀어야 한다.
* 검색 방법 : 주어진 데이터가 캐시에 있는지 어떻게 알아낼 것인가?
* 배치 정책 : 주어진 데이터가 캐시 어디에 자리 잡을 것인가?
* 교체 정책 : 캐시에 빈 공간을 어떻게 마련할까?
* 쓰기 정책 : 데이터를 쓰는 작업은 어떻게 다룰 것인가?

캐시 라인(cache line) 또는 캐시 블록(cache block)
* 캐시 엔트리와 같은 개념
* 이 캐시 라인에 저장되는 데이터의 단위 크기는 32, 64, 128 바이트가 일반적
  * 공간 지역성을 활용하기 위함
* 첫 번째 캐시 미스를 겪을 때 인접한 데이터도 같이 캐시에 올림으로 캐시 적중률을 올린다.
  * 이런 단위를 캐시 라인/블록이라 한다.
  * 여기에는 태그나 부가 정보가 덧붙는다.
* 많은 프로세서의 캐시 라인 크기가 64바이트이다.
  * 너무 크기가 작으면 공간 지역성 활용이 낮다.
  * 너무 크기가 크면 데이터를 가져오는데 시간이 오래걸린다.

## 캐시의 검색 방법
데이터 주소 값이 주어졌을 때 어떻게 해당하는 캐시 라인을 찾을 수 있을까?
* 32비트 프로세서에서 64바이트 캐시 라인일 때 캐시 라인은 6천만 개(2^32 / 2^6 = 2^26)가 넘는다.
  * 임의의 주소값이 전달되면 6천만 개의 캐시 라인 중 하나가 선택
* L1 캐시는 32KB, L2 이상은 MB 단위
  * 저장할 수 있는 캐시 라인 수는 수백 개에서 수만 개뿐
  * 따라서 6천만 개의 캐시 라인 주소를 캐시가 가진 수만큼 매핑할 수 있어야 한다.
* 이상적으로는 해시 테이블 같은 자료구조가 필요
  * 그런데 CPU 캐시는 소프트웨어와는 달리 단순히 배열로 구성
  * 소프트웨어의 복잡한 해시 함수를 만들기에는 비용이 크다.
* 따라서 주어진 주소를 단순 분해해서 캐시 자료구조로 매핑

캐시 자료구조의 구조

![12-03](https://github.com/MyungHyun-Ahn/SystemProgramming/assets/78206106/8c0427b4-52e8-4ca0-9c57-65ae56d93865)
* 태그(tag)
  * 최상위 비트 중 나머지 부분을 태그로 활용
  * 인덱스로 찾아진 캐시 라인이 정말 원하는 값인지 확인하는 용도
  * 수많은 주소 값이 같은 인덱스를 가지므로 확인 작업은 필요
* 인덱스(index)
  * 주소 값 중 가운데 부분을 떼어와 이 값으로 해당하는 캐시 라인을 찾는 것이 핵심
  * 예를 들어, 512개의 캐시 라인이 있다면 가운데 9비트 만큼이 인덱스에 해당
* 오프셋(offset)
  * 캐시 라인 내 원하는 데이터를 가리키는데 필요

64바이트 캐시 라인에서
* 6비트(26=64) 만큼이 오프셋 영역이 된다.
* 0~63 까지의 숫자로 64바이트 데이터 중 하나를 고른다.

인덱스를 가운데, 태그를 최상위 영역으로 택한 이유
* 캐시 매핑 시 충돌을 줄이기 위해
* 연속된 데이터에 접근할 때 최상위 부근의 숫자는 잘 바뀌지 않는다.

32비트 프로세서에서 64바이트 캐시 라인을 갖는 32KB 캐시 구조로 설명
* 32KB 캐시는 총 32 * 210 / 26 = 512 개의 캐시 라인을 저장할 수 있다.
* 따라서 오프셋은 6비트 인덱스는 9비트 태그는 15비트가 된다.
* 그리고 현재 라인이 유효한지를 가리키는 1비트의 valid 비트가 있다.

캐시 라인 시뮬레이터 코드
~~~C++
// 32비트 주소 공간, 64바이트 캐시 라인, 32KB 캐시
struct Data
{
  byte element[64];
};

struct CacheLine
{
  bool valid;       // 이 캐시 라인이 유효한가? (1비트)
  uint32_t tag;     // 태크 (15비트)
  Data data;        // 데이터 (64비트)
}

// 32KB 캐시 자료구조
CacheLine cache_[512];

// 주어진 주소 값에 해당하는 캐시 라인을 읽어온다.
CacheLine &LookupCache(uint32_t addr)
{
  // addr에 해당하는 인덱스를 구한다. 가운데 9비트만 추려낸다.
  uint32_t index = (addr >> 6) & 0x1FF;
  uint32_t tag = (addr >> (6 + 9));

  // valid가 켜져 있고 태그가 같아야 내가 원하는 주소다.
  if (cache_[index].valid == true && cache_[index].tag == tag)
    return cache_[index];
  else
    return invalid_cache_line;
}
~~~

## 캐시의 배치 정책
주어진 주소를 캐시에 저장할 때 남은 문제
* 같은 인덱스를 가지는 메모리 주소는 오직 한 곳에만 배치될 수 있다.
* 이런 구조는 같은 인덱스를 가지는 여러 개의 메모리 주소가 서로 충돌을 일으킬 수 있다.
* 충돌(conflict)을 최소화하고자 한 인덱스에 여러 개의 캐시 공간을 할당한다.

캐시 배치 정책
1. 직접 사상 캐시(directed-mapped) : 한 인덱스에 캐시 라인 하나 할당
2. 집합 연관 캐시(set-associative) : 한 인덱스가 N개의 캐시 라인 내에서 대응할 수 있는
3. 완전 연관 캐시(fully-associative) : 캐시 어디에도 할당할 수 있는

![12-04](https://github.com/MyungHyun-Ahn/SystemProgramming/assets/78206106/56c03fbf-322f-4533-9282-12dbc82cc556)

그림에서 집합 연관 캐시
* 하나의 인덱스가 두 개의 캐시 라인에 들어갈 수 있다.
  * 2-웨이(way) 집합 연관 캐시라 한다.
* N개가 가능하면 N-웨이 집합 연관 캐시
  * N을 연관도(associativity)라고 한다.
* N개의 캐시 라인은 하나의 세트(set)를 이룬다.
  * 직접 사상과 완전 연관은 N-웨이 집합 연관 캐시의 양 극단
* 2-웨이만 되더라도 캐시 충돌 미스를 없앨 수 있다.
* 현대 고성능 프로세서는 보통 8-웨이, 24-웨이 같은 연관도가 높은 캐시 구조도 쉽게 찾아볼 수 있다.

위의 캐시 시뮬레이터 직접 사상 캐시 코드를 4-웨이 집합 연관 캐시 구조로 바꾼 코드
~~~C++
// 32비트 주소 공간, 64바이트 캐시 라인 (4-웨이 집합 연관)
// CacheLine 구조는 동일
struct CacheSet
{
  CacheLine line[4];
};

// 32KB 4-웨이 캐시 자료구조 (128 = 512 / 4)
CacheSet set_[128];

CacheLine &LookupCache(uint32_t addr)
{
  // addr에 해당하는 세트 인덱스를 구한다. 가운데 7비트만 추려낸다.
  uint32_t set_index = (addr >> 6) & 0x07F;
  uint32_t tag = (addr >> (6 + 7));

  // 세트의 모든 캐시 라인과 비교한다.
  CacheSet &set = set_[set_index];
  for (int i = 0; i < 4; ++i)
  {
    if (set.line[i].valid && set.line[i].tag == tag)
      return set.line[i];
  }

  return invalid_cache_line;
}
~~~
* 64캐시 라인, 32KB 캐시인데 이제 4-웨이 집합 구조를 만들어야 한다.
  * 직접 사상에는 512개 세트, 여기서는 512 / 4 = 128개 세트
  * 인덱스 비트도 9비트에서 7비트로 줄인다.
* 세트 인덱스를 먼저 구하고, 이 세트에 있는 캐시 라인을 모두 태그로 검색해 원하는 데이터를 찾는다.
  * 여기서는 for문으로 구했는데 실제 하드웨어는 동시에 이루어진다.
  * 그만큼 전력과 트랜지스터 비용 증가
* 연관도를 높이면 캐시 충돌을 줄일 수 있어 좋으나 그만큼 비용이 발생하므로 적절한 타협점을 찾아야 한다.

캐시 배치 알고리즘
* 해시 테이블과 다를 것이 없다.
* 다만 소프트웨어의 해시 테이블처럼 가변 리스트를 가질 수 없어 고정된 길이의 리스트를 가진다.
* N-웨이 집합 연관 캐시는 각 원소마다 N개의 고정 길이를 가진 해시 테이블 구조로 보면 된다.

## 캐시의 교체 정책
같은 크기의 캐시에서 캐시 적중률(hit rate)은 캐시 연관도에도 영향을 미치지만
* 캐시 교체 정책(cache replacement policy)이 상당히 중요한 역할
* 직접 사상 캐시에서는 의미가 없다.
  * 어차피 인덱스 하나가 갈 수 있는 곳은 하나여서 이미 어떤 데이터가 그 자리를 차지하고 있다면 무조건 이 데이터가 삭제된다.
* 집합 연관 사상 캐시는 최대 N개 중 하나를 골라 없애야 하므로 어떤 정책을 쓸 것인지 결정해야 한다.

캐시에 빈 공간이 있다면 그 자리에 새로운 캐시 데이터를 채운다.
* 그렇지 않다면? 그 캐시 세트에 있는 데이터 중 하나를 내보내야 한다.
* 가장 단순한 방법 - 랜덤
  * 어떤 부가 정보도 기록할 필요가 없다.
  * 성능이 좋게 나올리 없다.
* 합리적인 정책 - 가장 오랫동안 사용되지 않을 캐시 라인을 내보내는 것
  * 미래에 일어날 일을 알아야 하므로 완벽한 구현이 불가능
  * 과거 정보로 추론할 수 있다.
  * 일반적으로 가장 최근에 사용되지 않은 것을 없애는 것이 효과적
    * LRU(Least Recently Used)
  * LFU(Least Frequently Used) : 캐시 라인의 접근 빈도를 따지는
  * FIFO(First In First Out) : 큐

이 정책의 소프트웨어 구현 방법은 매우 직관적
* LFU : 접근할 때마다 카운터 값을 증가
* FIFO : 큐 자료구조
* LRU : 캐시 라인마다 마지막으로 접근된 시간, 타임스탬프(time stamp)를 기록
  * 타임 스탬프 : CPU 사이클 값
  * 교체가 필요할 때, 캐시 라인의 타임 스탬프를 비교해 가장 작은 값 삭제

그러나 하드웨어에서 LRU의 구현 방식은 실현되기 어렵다.
* CPU 사이클은 32비트보다 큰 정수형이 필요
* 캐시 라인마다 이런 큰 카운터를 두는 것은 비용이 무척 크다.
* 다행히 N-웨이 집합 연관 캐시라면 각 캐시 라인마다 log2N 비트의 카운터만 있으면 LRU를 구현할 수 있다.

LRU 하드웨어 구현 알고리즘
1. 0 ~ N-1까지 가능한 카운터를 할당, ex) 16-웨이면 4비트 카운터
2. 캐시 라인이 접근되면 캐시 라인의 카운터 값을 잠시 기록하고, 이 값을 N-1로 바꾼다.
   * 같은 세트에 있는 다른 캐시 라인 중 카운터 값이 t보다 크면 1씩 감소시킨다.
3. 교체가 필요할 때, 카운터가 0인 캐시 라인을 찾아 삭제한다.

크기가 크고 여러 코어가 공유하는 L3 캐시는 웨이 수가 크므로 LRU를 구현하는 것이 쉽지 않다.
* LRU 카운터가 24-웨이면 5비트가 필요
* 카운터를 비교 갱신하는 비용이 크다.
* 따라서 유사(Pseudo) LRU 알고리즘을 사용
  * 캐시 세트에 있는 캐시 라인을 트리(tree) 구조로 관리
  * 가장 최근에 접근된(MRU) 캐시 라인으로 가는 경로를 기록하는 것

4-웨이에서의 유사 LRU 알고리즘

![12-05](https://github.com/MyungHyun-Ahn/SystemProgramming/assets/78206106/41e8222d-6544-46d1-944d-f55a5863ec8f)
* (a) : 웨이 1에 있는 캐시가 접근되었을 때, LRU 비트가 갱신되는 모습
  * 캐시가 접근될 때 비트 값 0은 왼쪽, 1은 오른쪽으로 간다.
* (b) : 캐시 라인이 교체될 때
  * 이 비트를 반대로 해석해 이동
  * 예제의 상태에서 교체가 필요하다면 웨리 3이 선택
  * 완벽하지는 않지만 적어도 가장 최근에 접근된 웨이 1은 선택되지 않는다.
* 이 방식은 많은 프로세서의 캐시 교체 정책

L1 캐시는 LRU 혹은 유사 LRU가 좋은 성능을 내는 것으로 알려짐
* 그러나 L2 이상의 캐시는 다른 정책을 쓰기도 한다.
* L2 캐시 접근의 형태가 L1 캐시 미스의 결과이므로 L1의 성격과 다르기 때문
* L1 캐시는 보통 LRU를 채택하고 있어 최근에 사용된 캐시 라인이 최대한 보존된다.
* 그 결과 L1 캐시로의 접근 패턴은 시간적 지역성이 L1에 비해 낮다.
* 그래서 L1 캐시가 아닌 캐시는 LRU 외의 정책을 쓰기도 한다.

## 캐시의 쓰기 정책
캐시에 데이터를 쓸 때 두 가지 정책
1. 라이트 쓰루(write-through) : 캐시 쓰기가 있을 때, 메모리 또는 아래 계층의 캐시로 바로 반영
   * 캐시를 쓸 때마다 아래 계층의 캐시나 메모리에 반영해야 하므로 레이턴시가 길다.
2. 라이트 백(write-back) : 변경된 데이터를 일단 캐시가 들고 있다가 나중에 캐시에서 쫓겨날 때 메모리를 갱신
   * 구현이 라이트 쓰루에 비해 복잡하다.
   * 캐시 라인마다 더티 비트(diry bit)를 둔다.
   * 이 값은 0으로 초기화되고 후에 쓰여지면 더티 비트를 켠다.
   * 갱신된 데이터가 캐시에만 있고 아직 메모리에는 반영되지 않았음을 의미
   * 캐시 라인이 교체되면 메모리에 바뀐 값을 쓴다.

대부분의 캐시는 라이트 백 쓰기 정책
* 예외적으로 L1 명령어 캐시는 라이트 쓰루
* 명령어 코드는 거의 수정이 일어나지 않기 때문
* Self-modifying code(SMC) : 자기 자신의 명령어를 고치는 프로그램, 컴퓨터 바이러스와 익스플로잇이 이 구조를 띠기도 한다.

캐시 쓰기 정책은 운영체제나 파일 입출력 프로그래밍에서도 쉽게 찾아볼 수 있다.
* 일반적인 하드디스크는 쓰기 캐시, 라이트 백 정책의 쓰기 캐시가 켜져있다.
* 하드 디스크에 바로 쓰지 않고 쓰기 캐시를 두고 데이터가 모이면 주기적으로 이 내용을 디스크로 보낸다.
* USB 처럼 빈번히 탈착되는 저장장치에 나쁘다.
  * 반드시 뽑을 때 안전한 하드웨어 제거를 거쳐야 한다.
  * 그래서 USB 메모리는 기본적으로 쓰기 캐시를 쓰지 않도록 설정된다.

## 고성능 캐시를 위한 알고리즘

캐시 성능을 정량적으로 표기하면
* 일반적인 캐시 성능은 평균 접근 시간으로 표현

평균 접근 시간 = 히트 레이턴시 + 미스 비율 = 캐시 미스 / (캐시 미스 + 캐시 히트) x 미스 패널티
* 이 공식에서 좌변을 낮추는 길은
* 히트 레이턴시, 미스 비율, 미스 패널티 항을 줄이는 것
* 세 값은 연관되어 있기도 한다.
  * 캐시 크기를 늘리면 당연히 캐시 미스는 줄어든다.
  * 무작정 캐시 크기를 크게 할 수 없다.
  * 캐시를 여러 계층을 두어 관리하는 것이 더 효과적

## 멀티 레벨 캐시
거의 모든 프로세서에서 캐시는 계층을 두고 있다.
* L1, L2, L3, ...
* 메모리 레이턴시를 줄이기 위해
* 캐시를 늘리면 캐시가 적중했을 때 데이터를 가져오는 비용이 커진다.
  * 예를들어 64KB 캐시가 데이터를 읽을 때 3사이클이 걸린다면
  * 8KB 캐시에서는 한 사이클에 가능할 수 있다.
* 캐시가 커짐에 따라 회로도 복잡해지고 배선 길이도 길어져서 이런 지연은 현대 반도체 기술로는 피하기 어렵다.
* 큰 캐시는 캐시 미스는 적지만 그렇지 않을 때는 더 많은 비용이 들어 평균 접근 시간은 손해볼 수 있다.
  * 따라서 L1 캐시는 히트 레이턴시가 더 중요한 설계 요소
  * 그래서 L1 캐시는 여전히 32KB, 64KB 수준에 머물러 있다.
* L1 캐시는 작고 빠르게 만드는 대신 속도는 다소 느린 L2 혹은 L3 캐시를 둠으로 미스 비율과 미스 패널티를 동시에 잡는다.

캐시가 여러 계층이 되면 생각해볼 문제
* 상/하위 캐시 간의 데이터를 중복해서 보관할 것인지 겹치지 않게 보관할 것인지에 대한 문제

세 가지 정책
* 인클루시브(inclusive, 포함하는) 캐시 정책
  * 어떤 캐시 라인이 L1에 있을 때, 반드시 L2 캐시에도 있다는 것을 보장
  * 단순히 존재 여부를 넘어 그 캐시 라인이 가지고 있는 값도 모든 캐시 레벨이 같게 한다.
* 익스클루시브(exclusive, 배타적인) 캐시 정책
  * L1에 데이터가 있다면 L2에는 반드시 없다.
* 논 인클루시브 또는 논 익스클루시브 캐시 정책
  * L1에 있을 때 L2에 있을 수도 없을 수도 있다.

정책에 따라 캐시 유효 공간이 차이가 나고 여러 캐시 연산의 복잡함과 단순함이 달라질 수 있다.

## 캐시 미스의 분류와 줄일 수 있는 방법
캐시 미스가 발생하는 이유는 네 가지로 나눌 수 있다.
1. 콜드 미스(cold miss, compulsory miss) : 데이터를 최초로 읽을 때 발생하는 캐시 미스
   * 캐시 라인 크기를 늘리거나 프리펫칭(Story 16)을 하면 줄일 수 있다.
   * 그러나 전체 용량이 같은 구조에서 캐시 라인이 지나치게 커지면 다른 충돌/용량 미스에 부작용을 미칠 수 있다.
2. 충돌 미스(conflict miss) : 캐시의 연관도(associativity)가 부족해 발생하는 캐시 미스
   * 캐시 연관도를 늘리는 것이 가장 효과적인 방법
   * 연관도를 늘린다고 콜드 미스나 용량 미스가 손해보는 것은 아니다.
   * 연관도가 너무 크면 히트 레이턴시를 악화시킬 수 있다.
   * N-웨이라면 모든 웨이가 주어진 데이터와 태그가 일치하는지 확인해야 한다.
3. 용량 미스(capacity miss) : 캐시의 용량이 부족해 발생하는 캐시 미스
   * 캐시 크기를 늘리는 방법 말고는 줄일 수 없다.
   * 캐시 용량을 늘려도 콜드 미스와 충돌 미스에는 영향이 없다.
   * 그러나 히트 레이턴시가 악화됨
4. 코히런스 미스(coherence miss) : 다른 프로세서에 의해 캐시 라인이 무효화되어 발생하는 캐시 미스
   * 잠시 뒤 알아본다.

빅틈 캐시(victim cache) - 캐시 충돌 미스를 줄이는 간단한 아이디어
* 아주 작은 규모의 완전 연관 캐시를 둔다.
* 이 작은 버퍼는 캐시 교체로 쫓겨난 몇몇 캐시 라인을 보관해 캐시 충돌 미스를 효과적으로 줄일 수 있다.
* 모든 캐시가 연관도가 높아야 하는 것이 아니라 일부 영역에서만 그러하다는 점에서 착안한 아이디어

## 그외 여러 테크닉
입출력 프로그램, 동기/비동기 방식

![12-06](https://github.com/MyungHyun-Ahn/SystemProgramming/assets/78206106/404b7320-864a-406e-83d1-22559fb4de9b)
* (a) : 동기 방식 - 파일 쓰기를 요청한 후 완료될 때까지 블록
* (b) : 비동기 방식 - 파일 쓰기를 요청만 하고 원래 프로그램은 다른 일을 하게 한다.
  * 작업이 완료되면 시스템은 완료 신호를 프로그램에 보낸다.
  * 이러한 기법을 레이턴시 감추기라 한다.

캐시 역시 이와 똑같은 문제가 있다.
* 옛날의 캐시는 미스를 겪으면 블로킹 상태가 되어 다른 캐시 접근 자체가 봉인
* 현대 캐시는 논 블록킹 캐시 구조가 일반적

캐시 역시 파이프라인화가 가능하다.
* 파이프라인은 대역폭을 개선하는 기술
* 캐시를 파이프라인화 하면 매 사이클마다 데이터 접근을 요청할 수 있다.
  * 특히 L1 캐시에서 필수적
  * GHz 급 프로세서에서 L1 캐시 레이턴시 조차 2~3사이클
* 캐시가 파이프라인화 되지 않는다면 캐시 요청마다 스톨이 발생해 성능이 크게 감소

프리펫처(prefetcher)
* 프로그램이 미래에 쓸 것 같은 데이터를 미리 캐시에 채워넣는 기술
* 캐시 미스 페널티 혹은 콜드 미스를 줄일 수 있다.
* Story 16에서 더 자세히 설명

## 멀티코어에서의 캐시
멀티코어에서의 캐시 구분
* 각 코어가 쓰는 전용 캐시(private cache)
* 여러 코어가 공유하는 공유 캐시(shared cache)

2010년의 AMD와 인텔 x86 프로세서의 캐시 구성도

![12-07](https://github.com/MyungHyun-Ahn/SystemProgramming/assets/78206106/cdf3b429-b7e3-4004-aee1-d4dfeab02a01)
* 각 코어에는 명령어 및 데이터 L1 캐시가 독립적으로 들어 있다.
* 그리고 L2 캐시 역시 독립 캐시로 구성
* 그러나 L3 캐시는 일반적으로 모든 코어가 공유하도록 설계한다.

코히런스(coherence) 문제
* 캐시에 저장된 데이터 사이의 일관성 유지 문제
* 멀티코어 캐시에서 풀어야 하는 가장 기본적인 문제
* 캐시 코히런스가 있어야 프로그래머는 멀티코어 환경에서 훨씬 쉽게 프로그래밍할 수 있다.
* 캐시 코히런스가 없다면 프로그래머가 명시적으로 캐시를 관리해야 한다.
  * 일반적인 SMP/CMP 환경에서 캐시 코히런스의 지원은 필수적

캐시 코히런스의 간단한 예
1. 코어 0이 100번지 데이터를 읽는다. 100번지 데이터는 코어 0의 전용 캐시에 있게된다.
2. 코어 1 역시 100번지 데이터를 읽는다. 100번지 데이터가 코어 1의 전용 캐시에 올라온다.
   * 두 코어 모두 읽기만 했으므로 각각 전용 캐시의 100번지 데이터는 서로 같다.
3. 그런데 코어 0이 100번지에 새로운 값을 쓴다고 하자.
   * 대부분의 캐시는 라이트 백 쓰기 정책이어서 100번지에 쓴 값은 코어 0의 캐시에만 최신 값이 있다.
4. 코어 1이 100번지 데이터가 다시 필요하다.
   * 만약 이 사실을 모른 채 자신의 캐시에서 100번지의 내용을 읽는다면 최신 내용을 읽어오지 못한다.

위와 같은 문제 떄문에 하드웨어 차원에서 캐시 코히런스를 지원해야 한다.
* 코히런스 문제는 멀티코어 캐시에만 있는 것이 아닌 모든 메모리 시스템이 겪을 수 있는 문제다.

## MSI 스누핑 프로토콜
MSU 스누핑(snooping) 프로토콜
* 캐시 코히런스의 가장 고전적인 해법 중 하나
* 캐시 라인 마다 세 가지 코히런스 MSI 상태를 가질 수 있도록 한다.
* 캐시에 접근할 때마다 모든 캐시에게 버스(bus)를 통해 신호를 보내는 스누핑 작업을 한다.

MSI 상태 세 가지
1. Invalid : 어떤 캐시 라인의 상태가 유효하지 않다.
   * 읽거나 쓰려면 반드시 값을 요청해야 한다.
2. Shared : 캐시 라인이 한 곳 이상에서 공유 중이다. 또는 자기 자신만 들고 있다.
   * 그러나 이 캐시 라인은 dirty 하지 않다.
   * 즉, 읽기만 한 상태
   * 메모리도 캐시 라인의 최신 값을 가지고 있다.
   * 어떤 프로세서가 쓰기를 하려면 반드시 신호를 보내 자신의 캐시 라인을 M 상태로 바꾸면서 다른 캐시 사본은 I로 바꿔야 한다.
3. Modified : 캐시 라인이 어떤 한 프로세서에 의해 고쳐졌음(dirty)를 뜩한다.
   * 메모리에는 이 값이 반영되지 않았을 수도 있다.
   * 캐시 라인이 M 상태라면 오직 하나의 코어만 이 캐시 라인을 가지고 있다.

## MESI 프로토콜
MSI 프로토콜에 추가적인 최적화로 버스에 신호를 보내는 횟수와 메모리 접근 횟수를 줄여보자.
* 프로세서 개수가 많아지면 버스 스누핑 트래픽도 증가해 병목 지점이 된다.
* MSI 프로토콜의 가장 큰 단점은 아무도 데이터를 공유하지 않아도 버스 트래픽이 낭비되는 점
* MESI 프로토콜은 이것을 해결한다.

MESI 프로토콜
* MSI 프로토콜에서 E(Exclusive) 상태를 추가한다.
* MSI에서 캐시 라인은 오직 자신만이 클린 상태이어도 S, 공유된 상태가 되어야 했다.
* S 상태에서 혼자만 깨끗한 데이터를 가지고 있는 상태를 E라고 한다.
* S 상태는 이제 반드시 두 개 이상의 캐시가 사본을 가지고 있는 경우가 된다.

## 더 생각해보기
MESI도 완벽하지 않다.
* 공유된 상태의 캐시 라인은 반드시 깨끗해야만 한다. 메모리 값과 일치해야 한다.
* 수정된 캐시 라인은 오직 한 프로세서만 공유할 수 있었다.
* 여러 프로세서가 빈번히 공유 데이터를 쓴다면 매번 더티 캐시 라인이 메모리로 반영되는 부담이 있다.

MOESI 프로토콜
* 더티 캐시 라인도 공유할 수 있게 허용한 프로토콜
* O(Owner) 상태 : 더티 캐시 라인도 공유되는데 이때 한 프로세서를 책임자, owner로 할당한다.
  * O 상태를 가진 프로세서가 최종적으로 라이트 백을 하도록 한다.
  * 이 상태의 불필요한 버스 트래픽을 더욱 줄일 수 있다.
* 이 프로토콜은 AMD의 캐시 구조에 쓰인다.1
  * 인텔은 MESIF 프로토콜
    * Forward 상태 : 캐시 라인을 달라는 요청에 응답

캐시 코히런스 작업의 확장성(scalability) 문제
* 8코어 정도에서는 MESI 혹은 변형 프로토콜이 잘 동작함
* 코어가 많아지면 잘 동작하지 않는다.
* 스누핑 기반의 캐시 코히런스는 비교적 작은 스케일에서 낮은 레이턴시로 작동할 수 있다.
* 더 큰 규모는 스누핑이 아닌 디렉토리 기반의 캐시 코히런스로 구현하기도 한다.
* 디렉토리 : 각 캐시 라인의 상태를 총괄하는 장치
  * 캐시 요청이 있으면 디렉토리에서 누가 가장 최신 값을 가지고 있는지를 알아낸다.
* 전체 프로세서에 데이터 요청 신호를 보내는 것이 아니기 때문에 비교적 확장성 있다.
* 그러나 레이턴시가 나쁘다.

많은 수의 코어에서 빠른 레이턴시로 캐시 코히런스를 구현하는 노력은 지금도 계속되고 있다.