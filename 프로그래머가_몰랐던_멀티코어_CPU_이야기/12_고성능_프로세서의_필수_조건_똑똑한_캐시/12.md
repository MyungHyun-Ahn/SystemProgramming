# Story 12 고성능 프로세서의 필수 조건 : 똑똑한 캐시
현대 범용 목적 마이크로프로세서의 회로 사진을 보면 절반 이상의 공간을 차지하는 부품이 CPU 캐시
* 파이프라인과 함께 프로세서의 성능을 높인 일등 공신

캐시가 필요한 이유
* 주 메모리인 DRAM에서 데이터를 가져오는 시간이 상당히 오래 걸리기 때문
* 자주 쓰고 인접한 내용을 고속의 캐시에 저장해서 명령어 완료 시간을 크게 단축

인텔 Nehalem 프로세서의 칩 모습
* 아래 있는 것이 L3 캐시, 각 코어의 절반 정도가 L1, L2 캐시에 해당
* 가장 자리에는 메모리 및 입출력 컨트롤러가 있다.

![12-01](https://github.com/MyungHyun-Ahn/SystemProgramming/assets/78206106/4480c500-c3c4-4dc7-afd4-23f251fecb8a)


## 왜 캐시가 필요하고 잘 작동할 수 있을까?
CPU 캐시가 필요한 이유
* 메인 메모리와 프로세서의 시간적, 공간적 거리가 멀어 데이터를 주고받는데 시간이 오래 걸리기 때문
* 그래서 자주 쓰고 인접한 데이터를 캐시에 보관해서 쓰는 것

일반적인 프로세서가 DRAM에서 자료를 요청해 받아오는데 보통 100사이클 이상
* 그런데 1~2사이클이면 레지스터 파일에서 데이터를 가져올 수 있다.
* 백 단위 사이클은 심각한 손실
* 프로세서와 메인 메모리의 클록 속도 차이도 문제
  * 2009년의 프로세서는 2~3GHz, 메모리는 800MHz
* 캐시가 절실한 이유 - 병목 지점이 DRAM, 디스크

온 칩, 오프 칩
* 현대의 CPU 캐시 : 온 칩(on-chip) 캐시, 칩 내부에 있는 것
* 과거의 CPU 캐시 : 오프 칩(off-chip) 캐시, 칩 외부에 있는 것

캐시는 CPU에만 있는 것이 아니다.
* 메모리 계층에서 속도, 공간, 가격 차이가 있는 두 계층 사이라면 캐시는 있을 수 있다.
  * 하드디스크의 캐시
  * DRAM 내의 행 버퍼(row buffer)
  * 소프트웨어에도 존재
  * 운영체제의 파일 시스템에도 여러 버퍼와 캐시
  * 웹 브라우저의 캐시

캐시가 잘 작동할 수 있는 원리 - 지역성(locality)
* 시간적 지역성(temporal locality)
  * 지금 어떤 데이터를 사용했다면 가까운 미래에 다시 사용할 확률이 높다는 것을 가리킴
  * 최근에 사용한 데이터를 보관, 가까운 미래에 이 데이터가 다시 사용되면 빠르게 줄 수 있다.
* 공간적 지역성(spatial locality)
  * 어떤 데이터가 사용된다면 그와 인접한 데이터에 접근할 것 같음을 뜻함
  * 데이터를 캐시에 넣을 때 그 데이터만 넣은 것이 아닌 그와 인접한 데이터도 함께 가져온다.
* 대부분의 데이터 접근이 랜덤하지 않고 지역성이라는 규칙을 띠어서 캐시가 잘 작동할 수 있다.

간단한 for 루프에서 시간적/공간적 지역성을 볼 수 있다.
~~~C++
1:  for (int i = 1; i < N; ++i)
2:    data[i] = data[i - 1] + 1;
~~~
* 시간적 지역성
  * 첫 번째 순환에서 값이 갱신되면 그 다음 순환에서 피연산자로 읽힌다.
* 공간적 지역성
  * data[1] 부터 data[N - 1]까지 접근

CPU 캐시는 데이터 접근의 두 지역성을 효과적으로 이용한다.

## 일반적인 캐시 구조
일반적인 소프트웨어에서 캐시
* 캐시는 접근 속도, 대역폭 단위 용량 당 가격 등 성능 차이가 뚜렷한 두 계층 사이에서 지역성을 활용해 자주 쓰이거나 인접한 자료를 잠시 저장한다.

캐시 모듈을 만들 때 필요한 인터페이스
1. 캐시에서 원하는 데이터를 찾는 함수
2. 새로운 데이터를 추가하는 함수
3. 이전 데이터 중 일부를 교체할 수 있는 정책이나 알고리즘
   * 캐시는 대부분 공간이 제한적이므로 캐시가 가득 찬 상태에서 새로운 데이터를 넣을 때

캐시에서 사용하는 용어
* 캐시 히트(cache hit) : 캐시에 원하는 데이터가 있을 때
* 캐시 미스(cache miss) : 캐시에서 원하는 데이터를 찾지 못했을 때
* 미스 패널티(miss penalty) : 캐시 미스가 발생하고 직접 해당 데이터를 찾아와 넣어야 하는 비용
* 히트 레이턴시(hit latency) : 캐시가 적중, 원하는 데이터를 가져오는데 드는 비용

매우 일반적인 소프트웨어 기반 캐시 시스템 구조를 그린 것

![12-02](https://github.com/MyungHyun-Ahn/SystemProgramming/assets/78206106/b6097ecc-2aab-4966-965c-9d1b42826ac7)

캐시는 여러 캐시 엔트리로 구성, 각 엔트리에는 실제 캐시하는 데이터와 부가 정보가 있다.
* 부가 정보는 일반적으로 다음으로 이루어진다.
  1. 실제 저장되는 데이터
  2. 태그(tag, 인식표)
  3. 캐시 찾기에 관련된 정보(포인터 같은 것)
  4. 캐시 교체에 관련된 정보(예, 언제 얼마나 이용되었는가)
  5. 기타 정보

주어진 데이터를 캐시에서 찾는 작업은 일반적으로 해시 테이블(hash table) 자료구조로 구현
* 캐시가 저장할 수 있는 캐시 엔트리의 개수는 가능한 전체 데이터 개수보다 보통 훨씬 작기 때문에
* 데이터를 받으면 해시 값을 구한 뒤, 해당 위치로 가서 탐색한다.

캐시 교체 알고리즘이 필요
* 오랫동안 쓰이지 않은 것을 삭제
* 교체 정책에 따라 자료구조가 다를 수 있다.

캐시를 설계
* 캐시 엔트리 구조를 정한다.
* 최적의 탐색 방법, 교체 정책을 정의한다.

CPU 캐시, 혹은 하드웨어 캐시도 기본적인 개념은 동일
* CPU 캐시는 일반적으로 SRAM(Static Random Access Memory)로 만들어진다.
  * DRAM에 비해 제조 비용이 비싸서 최대한 공간을 아껴야만 한다.
  * 해시 테이블처럼 복잡한 자료구조의 사용도 힘들고
  * 복잡한 해시 함수도 쓰기 어렵다.
* L3 캐시는 보통 eDRAM(embedded DRAM)을 이용하여 만든다.
  * SRAM 보다 레이턴시는 다소 느리지만 더 많은 용량을 구현할 수 있다.
* 캐시 교체에 필요한 부가 정보도 그 양을 줄여 비트 단위로 관리하고, 교체 알고리즘도 최대한 간단히 만들어야 한다.
  * 그렇지 않으면 캐시 레이턴시가 지나치게 길어진다.

## CPU 캐시의 기본적인 설계
CPU 캐시
* 메인 메모리와 레지스터 파일 사이의 큰 속도 격차를 줄이려고 프로세서 내에 있는 작지만 매우 빠른 기억 장치
* 데이터 접근의 시간적/공간적 지역성을 활용
* 캐시로 데이터를 가져오는데 필요한 메모리 레이턴시를 줄일 수 있다.

프로세서는 추상적으로 메모리 시스템이라는 장치에서 명령어와 데이터를 읽거나 쓴다.
* 캐시는 프로그래머의 입장에서는 있는지 없는지 모르게 처리, 특별한 최적화가 아닌 이상 고려할 필요가 없다.

메모리 시스템에는 레지스터와 DRAM이 있는데 이 사이에 캐시가 있어 큰 레이턴시 차이를 극복한다.

그런데 요즘 프로세서는 캐시가 여러 계층으로 있다.
* L1, L2 캐시 : 현대 프로세서 구조에서는 일반적
* L3 캐시 : 까지 있는 CPU도 쉽게 볼 수 있다.

보통 마지막 레벨의 칩 내 캐시를 특별히 LLC(Last Level Cache)라 한다.
* LLC 이후는 시간이 매우 오래 걸리는 칩 밖의 메모리 계층으로 이동해야 하기 때문에 특별히 구분

여러 계층의 캐시를 두는 이유
* 캐시 접근 속도와 용량 사이의 트레이드 오프가 있어 여러 단계를 두면 성능이 더 좋아지기 때문

프로세서가 메모리 시스템에 데이터를 요청하는 과정
* L1 캐시 부터 찾는다.
* 없다면 L2 캐시, 쭉쭉 내려가서 LLC 까지 찾는다.
* LLC에도 없다면 비로소 메인 메모리로 가서 데이터를 가져온다.
  * 이 데이터는 캐시에 보관되고 최종으로 프로세서에게 반환된다.

일반적으로 L1 캐시는 명령어와 데이터를 분리해 저장하는 것이 더 효과적이다.
* 많은 프로세서들이 이런 방식을 택한다.
* L2 캐시 이상은 분리하지 않는다.

캐시 설계는 다음 네 가지 문제를 풀어야 한다.
* 검색 방법 : 주어진 데이터가 캐시에 있는지 어떻게 알아낼 것인가?
* 배치 정책 : 주어진 데이터가 캐시 어디에 자리 잡을 것인가?
* 교체 정책 : 캐시에 빈 공간을 어떻게 마련할까?
* 쓰기 정책 : 데이터를 쓰는 작업은 어떻게 다룰 것인가?

캐시 라인(cache line) 또는 캐시 블록(cache block)
* 캐시 엔트리와 같은 개념
* 이 캐시 라인에 저장되는 데이터의 단위 크기는 32, 64, 128 바이트가 일반적
  * 공간 지역성을 활용하기 위함
* 첫 번째 캐시 미스를 겪을 때 인접한 데이터도 같이 캐시에 올림으로 캐시 적중률을 올린다.
  * 이런 단위를 캐시 라인/블록이라 한다.
  * 여기에는 태그나 부가 정보가 덧붙는다.
* 많은 프로세서의 캐시 라인 크기가 64바이트이다.
  * 너무 크기가 작으면 공간 지역성 활용이 낮다.
  * 너무 크기가 크면 데이터를 가져오는데 시간이 오래걸린다.