# Story 16 메모리 레이턴시 감추기 : 프리펫처
메모리 성능에 관한 이야기
* 지금까지 메모리 레이턴시가 수백 사이클까지 되므로 캐시가 아주 중요
* 메모리 명령 또한 최대한 비순차적으로 투기적 실행까지 동원하여 실행

또 다른 방법 - 프리펫칭
* 메모리 레이턴시를 감추는 셈
* 소프트웨어와 하드웨어에 모두 적용 가능

## 필요한 데이터를 미리 잘 가져오자
프리펫칭(prefetching)
* 메모리 레이턴시를 줄이는 방법
* 필요한 데이터를 예측해 미리 가져오는 방법

캐시 라인을 크게 하기
* 캐시 콜드 미스는 줄일 수 있었다.
* 이 콜드 미스를 최소화할 수 있는 다른 방법 중 하나가 프리펫칭 기법
  * 캐시 미스가 일어나지만 지연 시간을 감추는 것

프리펫처(prefetcher)의 아이디어
* 단순히 CPU와 메인 메모리 사이에서만 있을 필요도 없다.
* 소프트웨어 수준에서도 흔히 찾아볼 수 있다.

윈도우 운영체제에서의 프리펫칭 기능
* 슈퍼펫치(SuperFetch) : 자주 쓰는 프로그램을 기억해 미리 디스크에서 읽어 메모리에 올려놓는 기술
  * 단점 : 하드디스크를 쉬는 시간에도 읽는다.

프리펫칭의 단점
* 프리펫칭은 예측에 기반해 미리 데이터를 읽어오는 것
* 예측이 틀릴 수도 있다.
  * 낭비
* 소중한 대역폭을 갉아먹을 수 있다.
  * 특히 메모리 대역폭은 항상 부족
  * 실제 데이터 요청도 많은 상태에서 프리펫칭까지 하면 오히려 시스템에 악영향
* 따라서 프리펫칭은 함부로 하면 안 되고 정교하게 설계해야 한다.

프리펫칭의 예

![16-01](https://github.com/MyungHyun-Ahn/SystemProgramming/assets/78206106/f7c8910b-7261-4321-8cd2-87066f455bac)
* (a) : 프리펫칭이 없을 때
  * 데이터가 필요한 시점에 캐시 미스가 발생하고 메모리 접근
  * 그림에서 보다시피 40이라는 시간이 걸림
* (b) : 완벽한 프리펫칭이 작동할 때
  * 필요한 데이터 r1, r2, r3를 미리 요청
  * 정확히 필요할 때 타이밍을 맞추어 준비
  * 프로그램은 캐시 히트로 바로 데이터를 가져와 수행 시간을 크게 단축
  * 그러나 이렇게 완벽한 적중은 어렵다.
* (c) : 완벽하지 못한 프리펫칭의 경우
  * r1은 너무 늦게 프리펫칭이 요구 - 아무런 효과가 없다.
  * r2는 그보다는 빠르게 요구 - 어느정도는 줄일 수 있었다.
  * r3는 너무 빨랐음 - 프리펫칭으로 가져온 데이터가 캐시에서 쫓겨날 가능성
    * 그림에서는 쫓겨나지 않아서 메모리 레이턴시를 감추는데 성공
    * 만약 쫓겨난다면 프리펫칭은 악영향을 준다.
  * 기껏 프리펫칭한 데이터가 사용되지 않는다면
    * 메모리 대역폭과 캐시 용량만 낭비

캐시 오염(cache pollution)
* 캐시를 과도한 프리펫칭으로 낭비하는 것

## 기본적인 소프트웨어 프리펫칭
소프트웨어 프리펫칭
* 프로그래머나 컴파일러가 ISA가 정의하는 명령어로 프리펫칭을 지시하는 기법
* 프리펫칭 명령어의 형태는 메모리 로드와 상당히 흡사
  * 프리펫칭할 주소 값을 인자로 주면 캐시에 데이터를 읽는다.
  * 주소 값외에 어떤 캐시로 읽을지도 정할 수 있다.
  * 메모리 로드와 다른 점
    * 페이지 폴트와 같은 예외를 만들지 않는다. - 예외가 발생해도 무시
    * 알고리즘에 영향을 주면 안되기 때문
  * 명령어가 추가로 들어가므로 오버헤드가 발생
    * 명령어 캐시, 파이프라인에 부담
  * 따라서 프리펫칭 요구는 우선순위가 낮게 책정

프리펫칭 명령어
* x86 : PREFETCH
* 인트린직 : _mm_prefetch() 함수

프리펫칭 코드를 넣을 코드
~~~C++
1:  for (i = 0; i < N; i++)
2:      ip = ip + a[i] * b[i]; // double a[], double b[]
~~~

프리펫칭을 실시하는 시점은 매우 중요하다.

먼저 k 번째 순환일 때 k + 1의 데이터를 읽는 코드
~~~C++
1:  for (i = 0; i < N; i++) {
2:      prefetch(&a[i+1]);
3:      prefetch(&b[i+1]);
4:      ip = ip + a[i] * b[i];
5:  }
~~~

이 코드에서의 문제점
* a[0], b[0]에 대한 프리펫칭이 없다.
* ip 변수 역시 프리펫칭을 하지 않았다.
* 마지막 순환에서 a[N], b[N]에 대해 불필요한 프리펫칭 요구를 한다.
* 이 코드는 캐시 라인 크기를 전혀 고려하지 않는다.

프리펫칭을 정확히 하려면
* 그 코드가 어떤 캐시 미스 형태를 만들어낼지 미리 예측해야 한다.
* 즉, 기본적인 캐시 정보는 알아야 한다.

캐시 라인 크기를 32바이트로 가정
* 4개의 double 원소가 들어갈 수 있다.
* 루프의 네 번째 순환마다 캐시 미스를 만들 것
* 분명히 매 순환마다 프리펫칭하는 것은 낭비

간단한 해법 - 캐시 크기를 고려해 4번 순환마다 프리펫칭
~~~C++
1:  for (i = 0; i < N; i++) {
2:      if (i % 4 == 0) {
3:      prefetch(&a[i+1]);
4:      prefetch(&b[i+1]);
5:      }
6:      ip = ip + a[i] * b[i];
7:  }
~~~

이 방법 또한 바람직하지 않다.
* if 문 추가, % 명령, 비교 명령
* 늘어난 명령어와 분기문으로 프리펫칭의 효과가 상쇄

위 문제를 해결하기 위한 방법 - 루프 풀기
~~~C++
1:  for (i = 0; i < N; i += 4) {
2:      prefetch(&a[i+4]);
3:      prefetch(&b[i+4]);
4:      ip = ip + a[i] * b[i];
5:      ip = ip + a[i+1] * b[i+1];
6:      ip = ip + a[i+2] * b[i+2];
7:      ip = ip + a[i+3] * b[i+3];
8:  }
~~~

아직 남아있는 단점
* a[0] ~ a[3] 까지의 데이터는 프리펫칭되지 않는다.
* 마지막 순환에서도 불필요한 프리펫칭이 발생한다.

모든 캐시 미스에 대한 프리펫칭
~~~C++
1:  prefetch(&ip);    // 프롤로그
2:  prefetch(&a[0]);
3:  prefetch(&b[0]);
4:
5:  for (i = 0; i < N-4; i += 4) { // 메인 루프
6:      prefetch(&a[i+4]);
7:      prefetch(&b[i+4]);
8:      ip = ip + a[i] * b[i];
9:      ip = ip + a[i+1] * b[i+1];
10:     ip = ip + a[i+2] * b[i+2];
11:     ip = ip + a[i+3] * b[i+3];
12: }
13:
14: for (; i < N; ++i) // 에필로그
15:     ip = ip + a[i] * b[i];
~~~
프롤로그와 에필로그를 넣어 문제를 해결
* 먼저 ip, a[0], b[0]에 대해 미리 프리펫칭
* 불필요한 프리펫칭은 루프 순환 횟수를 한번 줄임으로 가능

이제 남은 문제 - 지금까지는 한 순환에 앞서 데이터 요청
* k번 째 순환에서 k+1에 있는 데이터를 미리 요청
* 일반적으로 k번 순환할 때 k+d에 있는 데이터를 프리펫칭
  * d를 프리펫칭 거리라고 한다.
* 최적의 거리는 메모리 레이턴시를 감출 수 있을 만큼 미리 수행되어야 함
  * 이 프로세서의 메모리 레이턴시를 100사이클로 가정
  * 메인 루프의 한 순환이 40사이클이라 하면
  * 100 / 40 = 2.5 순환만큼이 메모리 레이턴시를 감추는 데 필요
  * 이 거리는 소수가 될 수 없으므로 올림해서 3을 얻는다.
  * 따라서 k번째 순환이라면 k + 3의 데이터를 미리 요청해야 한다.

d=3으로 수정한 코드
~~~C++
1:  prefetch(&ip);    // 프롤로그
2:  for (i = 0; i < 12; i+=4) {
3:  prefetch(&a[i]);
4:  prefetch(&b[i]);
5:  }
6:
7:  for (i = 0; i < N-4; i += 4) { // 메인 루프
8:      prefetch(&a[i+12]);
9:      prefetch(&b[i+12]);
10:     ip = ip + a[i] * b[i];
11:     ip = ip + a[i+1] * b[i+1];
12:     ip = ip + a[i+2] * b[i+2];
13:     ip = ip + a[i+3] * b[i+3];
14: }
15:
16: for (; i < N; ++i) // 에필로그
17:     ip = ip + a[i] * b[i];
~~~

## 포인터 기반 자료구조의 소프트웨어 프리펫칭
포인터 기반 자료구조
* 링크드 리스트, 해시 테이블, 트리, 그래프 등등

이런 포인터 기반 자료구조에서는 어떻게 프리펫칭할까?

간단한 링크드 리스트 순회 코드
~~~C++
1:      struct Node {
2:          int data;
3:          struct Node *next;
4:      };
5:
6:      Node *p = ...;
7:      while (p) {
7a: cmp qword ptr [p], 0        ; p와 0을 비교
7b: je 11                       ; 만일 0이면 11번으로
8:          work(p->data);
8a: mov rax, qword ptr [p]      ; p를 로드, 캐시 미스
8b: mov ecx, dword ptr [rax]    ; p->data를 읽음
8c: call    work                ; work 호출
9:          p = p->next;
9a: mov rax, qword ptr [p]      ; 미 최적화로 남은 중복 코드
9b: mov rax, qword ptr [rax+8]  ; p->next를 로드
9c: mov qword ptr [p], rax      ; p->next의 값을 p에다 다시 씀 
10:     }
10a:jmp 7a                      ; 7a의 위치로 돌아가라
11:
~~~
프피펫칭이 없다면
* p의 명령을 읽는 8a 명령어에서 캐시 미스가 발생
* 따라서 p가 가리키는 내용을 프리펫칭해야 한다.

위 소스를 시각화 한 그림

![16-02](https://github.com/MyungHyun-Ahn/SystemProgramming/assets/78206106/0d576c67-10af-4bae-9bef-c0071ff47b10)

프리펫칭 거리를 구하려면 메모리 레이턴시와 루프 순환의 길이를 고려해야 한다.
* 메모리 레이턴시가 Work 함수 수행 시간의 3배라고 가정
* 프리펫칭이 없다면 한 루프 순환은 L+W의 시간이 걸린다.
* L이 W의 3배이므로 k번 원소를 처리할 때 k+3번의 원소를 프리펫칭하면 된다.

위 소스의 두 가지 프리펫칭 버전
~~~C++
// 프리펫칭 a
1:  while (p) {
2:      prefetch(p->next->next->next);
3:      work(p->data);
4:      p = p->next;
5:  }

// 프리펫칭 b
1:  while (p) {
2:      prefetch(p->next);
3:      work(p->data);
4:      p = p->next;
5:  }
~~~
프리펫칭 a
* 프리펫칭 거리가 3
* 그런데 배열과는 달리 포인터 자료구조는 바로 거리 3의 노드를 구할 수 없다.
* p->next->next->next가 모두 로드 의존성(load-to-load dependence)이 있기 때문

포인터 추적 기계어 코드
~~~C++
1:      Node *next3 = p->next->next->next;
1a: mov rax, qword ptr[p]
1b: mov rax, qword ptr[rax+8]
1c: mov rax, qword ptr[rax+8]
1d: mov rax, qword ptr[rax+8]
1e: mov qword ptr [next3], rax
~~~
* 3칸 앞서있는 노드를 구하려면 연속으로 4번 로드해야 한다.
* 앞 로드의 결과가 다음 로드의 인자로 사용되어 순차적으로 실행되어야 하므로 오버헤드가 크다.
  * 프리펫칭 자체를 위한 연산 오버헤드는 절대 바람직하지 않다.
  * 포인터 코드는 여기에 로드 의존성까지 더해져 쉽지 않다.

위 프리펫칭 예시에서 a보다 b가 낫다.
* 결과적으로는 두 코드는 큰 차이를 보이지 않고 L 시간이 더 걸릴 것이다.
* 특히 p->next->next->next와 같은 코드는 꼬리를 물며 접근하는 코드로 포인터 추적 코드라고 한다.
  * 프리펫칭 하기 어려운 대표적인 부류

### 그리디 프리펫칭
포인터 자료구조를 위한 첫 번째 프리펫칭 알고리즘

그리디(greedy) 전략
* 그 노드에 인접한 모든 노드를 미리 읽는 것
* 완벽한 프리펫칭은 안되더라도 메모리 레이턴시는 조금 줄일 수 있다.

이진 트리를 전위 탐색 하는 함수에 적용
~~~C++
1:  void preorder(TreeNode *t) {
2:      if (t != NULL) {
3:          prefetch(t->left);
4:          prefetch(t->right);
5:          process(t->data);
6:          preorder(t->left);
7:          preorder(t->right);
8:      }
9:  }
~~~
process 함수가 데이터 레이턴시의 약 1/2라고 가정

전위 순회시 캐시 적중이 어떻게 되는지 보이는 그림

![16-03](https://github.com/MyungHyun-Ahn/SystemProgramming/assets/78206106/c5c8b098-41b0-4378-af8c-61b2db4cdf35)
* 그리디 프리펫칭으로 어떤 노드는 완벽하진 않지만 메모리 레이턴시를 줄일 수 있다.

전위 순회 분석
* 위 예시에서 프리펫칭이 일찍 일어나도 캐시에서 쫓겨나지 않는다 가정
* 왼쪽 자식은 프리펫칭이 늦어 부분 캐시미스
* 오른쪽 자식은 캐시 히트

그리디 프리펫칭 정리
* 프리펫칭 거리를 조절하기는 어렵지만
* 구현이 쉽고 어떠한 포인터 기반 자료구조에도 적용이 가능하다.
* 오버헤드 또한 적다.

### 히스토리 포인터를 이용한 프리펫칭
보다 정확한 프리펫칭 거리를 제어할 수 있는 프리펫칭 방법
* 각 노드가 프리펫칭 거리인 d만큼 앞서 노드를 가지고 있다면
* 이 포인터 노드를 바로 프리펫칭 함으로 프리펫칭 오버헤드는 일반 포인터와 같으면서 정확한 프리펫칭을 할 수 있다.
* 일종의 점프 포인터를 만드는 방법
  * 일단 어떤 자료구조가 처음 순회될 때 순회 내력을 기억해서 프리펫칭 거리만큼 떨어져 있는 노드를 서로 이어주자.

히스토리 포인터
* 추가로 기억하는 포인터
* 이를 이용한 방법은 주어진 자료구조가 자주 변경되지 않아야 하고 같은 패턴으로 순회가 자주되어야 한다.

히스토리 포인터를 이용한 전위 탐색의 예

![16-04](https://github.com/MyungHyun-Ahn/SystemProgramming/assets/78206106/8efac6ec-fbd6-4326-86c9-ba401d625e4a)
* 트리 노드는 히스토리 포인터를 하나 더 가진다.
* 히스토리 포인터는 자료구조가 최초로 순회될 때 갱신
* 만일 프리펫칭 거리가 d라 하면 d개의 원소를 가질 수 있는 큐를 하나 둔다.
* 노드를 방문할 때마다 큐에 넣자.
* 이제 노드를 방문할 때 큐에서 가장 오래된 노드의 점프 포인터를 현재 방문하는 노드로 갱신하면 된다.

히스토리 포인터 방식
* 자주 바뀌지 않으며 같은 패턴으로 반복 순회하는 포인터 기반 자료구조에 더욱 효과적
* 대신 포인터 공간이 더 필요하며 히스토리 포인터를 관리하는 오버헤드가 든다.

### 데이터 선형화를 이용한 프리펫칭
적용할 수 있는 경우는 한정적이지만 극단적으로 포인터 기반 자료구조의 프리펫칭 비용을 연속적인 메모리를 쓰는 배열처럼 만드는 것

데이터 선형화를 이용한 가속 방법 그림

![16-05](https://github.com/MyungHyun-Ahn/SystemProgramming/assets/78206106/347a6829-c385-498e-a48e-c176a3563653)
* 트리 자료구조를 선형화시킨 것
* 즉, 순회하는 순서대로 각 노드를 메모리에 할당한 것
* 이렇게 하면 단순히 배열 인덱스 구하듯이 주소를 구해 프리펫칭하면 된다.

그러나 이 방법은 매우 제한적으로 사용되어야 한다.
* 이미 생성된 자료구조의 메모리를 이동하는 것은 매우 위험하고 비용이 크다.
  * 메모리를 가리키는 포인터를 모두 알아내 바꿔야 한다.
* 처음 자료구조를 만들 때 순회 순서를 미리 알 수 있다면 그 순서에 맞춰 메모리에 배치하면 이 방법을 활용할 수 있다.

## 하드웨어 프리펫칭 알고리즘
하드웨어 프리펫칭
* 메인 메모리와 캐시 사이에서 일어나는 행동을 관찰
* 만일 캐시 미스가 발생하고 데이터 접근의 규칙을 발견하면 프리펫칭 알고리즘에 의해 프리펫칭 요구를 메인 메모리로 생성할 수 있다.

하드웨어 프리펫처
* 여러 정보를 이용해 미래에 일어날 일을 예측하는 장치
* 프로세서가 요청한 메모리 주소, PC, 메모리 값, 예전 메모리 주소, 예전 메모리 값 등을 취합해 프리펫치할 주소와 타이밍을 결정하는 작업
* 현대 고성능 프로세서에는 필수적으로 들어가 있는 장치

캐시 자체의 프리펫칭 효과
* 캐시 라인 크기가 64바이트처럼 큰 이유
  * 공간적 지역성을 살리기 위해서
  * 인접한 데이터를 프리펫칭하는 것과 같다.
* 캐시 블록이 너무 커지면 캐시 오염의 위험도 커진다.
* 가짜 공유(false sharing)라는 문제도 빈번해진다. - Story 20

### 순차적 프리펫칭과 스트라이드 프리펫칭
가장 단순한 하드웨어 프리펫처
* 인접한 캐시 라인을 프리펫칭 하는 것
* 캐시 라인 크기를 두 배 늘리는 것과 유사하다.
* 대표적으로 명령어 캐시에 적용할 수 있다.
  * 분기가 없다면 항상 순차적으로 접근
  * 프리펫칭을 요구하여 명령어 투입 속도를 늘릴 수 있다.

위 방법을 확장하면
* 어떤 캐시 라인이 접근될 때 인접한 k개의 데이터를 가져오는 것
* 그러나 k가 너무 커지면 캐시 오염이 심해진다.
* 이것을 해결하는 방법 큐 형태의 스트림 버퍼(stream buffer)를 이용하는 방법
  * 이 방법은 캐시 오염 문제를 일으키지 않는다.
  * 하지만 포인터 기반 자료구조라면 프리펫칭 효과는 기대할 수 없다.
  
스트림 버퍼를 이용하는 방법의 과정  
1. 프리펫칭 한 데이터를 캐시 라인에 넣지 않고 일단 스트림 버퍼에 저장
2. 스트림 버퍼에 있는 내용이 접근되면 캐시로 옮겨진다.
3. 새로 생긴 스트림 버퍼의 공간에는 또 다른 캐시 라인이 프리펫치된다.

스트라이드(stride) 프리펫칭
* 메모리 주소를 관찰해 이전 주소와 현재 접근하는 데이터 주소의 차이를 기억하는 방법
* 스트라이드 : 두 주소의 차이
* 어떤 로드 명령어가 폭이 일정한 메모리 접근을 한다면 그 정보를 배워 프리펫칭에 쓴다.
* 대표적으로 행렬 곱셈에서 효과를 볼 수 있다.

### 마르코프 프리펫칭 알고리즘
이중 캐시 미스를 일으키는 메모리 주소의 스트림을 관찰해 마르코프(Markov) 모델을 이용한 프리펫칭 알고리즘

미스 스트림
~~~
A, B, C, D, C, E, A, C, F, F, E, A, A, B, C, D, E, A, B, C, D, C
~~~
* 캐시 미스 스트림의 예
* 데이터 주소 A에서 미스가 일어나면 그 다음 캐시 미스가 주소 B에서 일어난다는 이야기
* 마르코프 프리펫처는 이런 스트림의 마르코프 모델을 만든다.

각 상태로의 전이 확률을 기록한 마르코프 모델

![16-06](https://github.com/MyungHyun-Ahn/SystemProgramming/assets/78206106/c0723d92-0e81-4654-b9a7-57683688bd68)
* 이 정보를 활용하면 다음에 똑같은 프로그램이 실행된다는 가정 아래
* 캐시 미스 A를 보았을 때 A, B, C 중 하나를 택할 수 있다.
* 그런데 히스토리를 두 개로 늘린다면, A, B 뒤에는 100%로 C가 왔다는 예측도 가능
  * 그러나 히스토리를 늘리는 것은 낫지 않다고 알려져 있다.
* 즉, 히스토리를 하나만 보는 경우 다음 문제만 생각하면 된다.
  * "주소 X가 미스가 났는데 다음에는 어떤 주소가 캐시미스날까?"

그런데 하드웨어는 공간, 시간의 제약으로 그림 같은 그래프를 만들기 어렵다.
* 따라서 근사화된 마르코프 모델을 만든다.
* 마르코프 프리펫처는 캐시 같은 테이블이 필요하다.
  * 데이터 주소 값에 해당하는 인덱스를 찾아 뒤따르는 미스 주소를 기록하면 된다.

마르코프 프리펫처 테이블

![16-07](https://github.com/MyungHyun-Ahn/SystemProgramming/assets/78206106/d9734b42-dd3a-4a83-ba9e-9d84d05d4be4)
* 한 미스 주소 인덱스에 최대 4개의 다음 미스 주소를 기록
* 프리펫처 테이블에 더 많은 공간이 투자될 수 있다면 당연히 이 양은 늘어날 수 있다.
  
완벽한 마르코프 모델
* 확률 값을 가지고 LRU나 LFU를 이용해 확률을 근사할 수 있다.
* 캐시 미스 스트림을 볼 때마다 적절히 테이블을 갱신하면 된다.
* 그리고 어떤 캐시 미스 주소가 프리펫처 테이블의 엔트리와 일치하면 그 엔트리가 가지고 있는 예측 주소 값 집합에 대해 프리펫칭을 실시하면 된다.
* 여기서 확률 근사 값에 따라 프리펫칭 우선순위를 조절할 수 있다.

마르코프 방식 정리
* 어떤 주소 다음에 뒤따르는 주소들의 관계를 기억하여 프리펫칭 정확도를 높이는 것
* 단점 : 반드시 학습 단계, 테이블을 채우는 단계가 필요하다.
  * 이 시점의 캐시 미스를 잡기는 어렵다.

## 내용 기반의 하드웨어 프리펫처
내용 기반의 하드웨어 프리펫처
* 캐시 미스 주소가 아닌 직접적으로 데이터 '내용(content)'를 읽어 이 값이 포인터 같으면 나중에 사용된다는 기대를 가지고 프리펫칭하는 알고리즘
* 포인터 기반 자료구조를 보면 포인터 값을 읽고 읽은 내용은 다음 로드할 주소가 된다.
* 만일 하드웨어가 어떤 데이터의 내용을 엿봐서 유효한 가상 메모리 주소 값이 같으면 프리펫칭을 하는 아이디어

메모리 주소 그림
![16-08](https://github.com/MyungHyun-Ahn/SystemProgramming/assets/78206106/d764d924-ecbf-434b-95e5-0f5f9a5d2d67)
* 캐시 라인을 읽고 내용을 살펴본다.
* 첫 번째 두 번째 워드는 데이터 주소같지 않다.
* 그런데 나머지 두 개는 메모리 주소 값 같아보이므로 프리펫칭한다.

도대체 어떤 값이어야 데이터 주소가 같다고 판단할 수 있을까?
* 메모리 동적 할당 형태를 이용
* 힙 관리자를 통해 비슷한 시기에 할당받은 포인터는 상위 영역이 상당 부분 겹친다.
* 따라서 이 데이터 값이 많이 겹치면 포인터로 추측할 수 있다.

내용 기반 프리펫처의 장점
* 학습 시간이 필요하지 않아 포인터 기반 자료구조가 많이 쓰이는 코드에서 좋은 성능을 보인다.

그 외의 기법
* 헬퍼 스레드 기법
  * 프로그램의 일부를 앞서 실행시켜 캐시 미스가 미리 처리되게 하는 기법