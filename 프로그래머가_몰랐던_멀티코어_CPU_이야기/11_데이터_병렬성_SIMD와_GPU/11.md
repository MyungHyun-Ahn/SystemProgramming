# Story 11 데이터 병렬성 : SIMD와 GPU

고성능 프로세서를 향한 방법 중 가장 큰 원리
* 다양한 병렬성을 찾아 활용
* 이번 장에서는 데이터 병렬성이라는 아주 쉽고 간단한 원리를 이야기

## 데이터 병렬성
데이터 수준 병렬성(Data-level parallelism)
* 가장 쉽게 설명할 수 있는 것은 벡터 연산

4차원 두 벡터의 합 코드
~~~C++
1:  for (int i = 0; i < 4; ++i)
2:      c[i] = a[i] + b[i];
~~~
* 벡터 원소에 대한 작업은 서로 독립적
* 동시에 실행될 수 있다.
* 이것이 바로 데이터 병렬성
* 이런 작업을 SIMD라고 한다.

SIMD(Single Instruction Multiple Data)
* 같은 명령어가 다른 데이터에 각각 적용된다는 뜻
* 주로 과학, 산술, 그래픽 작업에서 자주 등장하는 연산
* 이런 부분을 가속하기 위한 하드웨어와 명령어가 고안

x86의 SSE(Streaming SIMD Extension) 명령어 구조
* 벡터 합 연산을 기계어 하나로 간단히 해결 가능

~~~asm
1:  VECTOR_ADD C, A, B
~~~
* SIMD 덧셈 명령어
* x86의 SSE는 128비트의 데이터 형을 지원
* SSE 레지스터 8개의 xmm0~7 레지스터를 제공
* 128비트는 보통 팩(pack) 형태로 구성

SSE 버전 4.2 128비트 레지스터에 담을 수 있는 값
* 4개의 float 데이터
* 2개의 double 데이터
* 4개의 32비트 정수
* 8개의 16비트 정수
* 16개의 1바이트 정수

SIMD 처리에 특화된 ALU가 따로 있다.

벡터 합 SIMD 명령어 실행구조

![11-01](https://github.com/MyungHyun-Ahn/SystemProgramming/assets/78206106/2c94fdcf-9474-48f6-a1ee-b4f3d1888c21)
* 두 피연산자 벡터가 SIMD 레지스터 A, B에 올라와 있다.
* 이것이 SIMD 연산장치로 가면 동시에 덧셈이 이뤄져 목적지 레지스터 C에 쓰이는 구조

SIMD 지원 연산
* 기본 덧셈, 뺄셈, 곱셈
* 비교, 변환, 셔플(shuffle), 비대칭 연산, 매스크 연산 등

자동 벡터화(automatic vectorization)
* 컴파일러가 코드를 분석하여 적합하면 자동으로 SIMD 명령어를 대체하는 최적화

프로그래머가 직접 SIMD 명령어를 쓰려면
* 컴파일러가 제공하는 인트린직(intrinsic)이라는 기계어와 1:1 대응되는 함수 사용

SIMD는 계속 진화하고 있다.
* x86 샌디 브릿지 마이크로아키텍처에서는 256비트 벡터 연산 지원하는 명령어 AVX(Advanced Vector Extension) 추가

## GPU : 또 하나의 병렬 프로세서
비디오(그래픽) 카드
* CPU가 잘 처리하지 못하는 그래픽 작업을 빠르게 처리하고자 탄생
* 원래 2차원만 처리하다 1990년대 후반부터 3차원 가속 기능 탑재

3차원 그래픽 카드
* 그래픽 처리 파이프라인의 특화
* 처음에는 고정된 기능만 하드웨어로 제공
* 픽셀 쉐이더 같은 기능으로 프로그래밍할 수 있는 기회 제공
  * 더욱 세밀한 3차원 그래픽 처리 과정을 제어
* GPU는 사실 데이터 병렬성이 많은 작업을 처리하는데 아주 적합한 구조

DirectX나 OpenGL 같은 그래픽 가속 API
* GPU의 접근하는 통로가 기존에는 제한적
* 그래픽 가속 API의 등장으로 비디오 카드를 제어 가능
* 이제는 그래픽 연산을 넘어 범용 목적의 병렬 계산 프로세서로 쓸 수 있다.

범용 목적 GPU(General Purpose Graphic Processing Unit)
* GPGPU
* nVidia가 C언어 기반의 CUDA(Compute Unified Device Architecture)라는 환경을 내놓으며 GPGPU 열풍 시작

일반적인 컴퓨터의 구성도

![11-02](https://github.com/MyungHyun-Ahn/SystemProgramming/assets/78206106/89de6447-7e9e-4602-a605-83bef03f6bd6)
* 비디오 카드는 컴퓨터 메인보드의 칩셋이라는 모듈의 도움을 받아 버스(bus)로 CPU와 데이터를 주고 받는다.
* 버스는 프로세서와 주변 기기가 서로 데이터를 주고받을 수 있는 통로
* GPU도 이런 버스로 CPU와 연결되어 있다.

CPU에게 메인 메모리가 있듯이 비디오 카드도 자신만의 비디오 램이 있다.


CPU와 GPU의 구조

![11-03](https://github.com/MyungHyun-Ahn/SystemProgramming/assets/78206106/6d5d0093-29cb-4c4a-870a-7ca2e9481c49)

CPU
* 명령어 처리 자체를 위한 장치가 많다.
* 특히 캐시는 매우 중요
* 그런데 ALU는 개수가 매우 적다. - ILP가 제한적이므로
* 대신 명령어 처리 레이턴시를 낮추는 것이 목적이므로 트랜지스터를 캐시와 컨트롤에 활용

GPU
* 대부분의 자원을 ALU에 투자
* GPU의 가장 큰 사용 목적은 고성능 3차원 게임에 있었다.
* GPU가 처리하는 명령어는 데이터 병렬성이 매우 풍부 - 컨트롤 장치가 많이 필요하지 않다.
* 따라서 계산 장치에 대부분의 트랜지스터를 할당
* 실제로 현재 모든 GPU는 순차 방식으로 처리된다.
* GPU는 수천, 수만 개의 스레드 문맥을 관리할 수 있다.
* 스레드 문맥 교환 비용도 CPU에 비하면 아주 낮다.
* 아주 큰 레지스터 파일이 필요

GPU는 메모리 대역폭이 CPU보다 훨씬 높아야 한다.
* 멀티코어 프로세서에서 메모리 장벽은 심각한 문제
* GPU는 코어가 훨씬 많아 그만큼의 데이터를 메모리에서 가져와야 함
* GeForce GTX 280 GPU = 160GB/s 엄청난 메모리 대역폭


## nVidia GPU 구조

nVidia 8800GTX GPU의 구조

![11-04](https://github.com/MyungHyun-Ahn/SystemProgramming/assets/78206106/3de73b70-307c-47ae-9382-814c3cb57ebd)
* 호스트 : CPU, 디바이스 : GPU
* CPU가 GPU에 실행할 코드를 보내면 수많은 코어가 동시에 처리
* 이 병렬 프로세서는 여러 계층을 이루며 구성
* 16개 스트리밍 멀티프로세서(Streaming Multiprocessor, SM)
* SM이 2개 씩 모여 텍스처 프로세서 클러스터(Texture Processor Cluster, TPC)
* 두 SM마다 텍스처라는 장치 : 일종의 캐시
  * 그래픽 처리에서 텍스처 장치는 중요하기 때문에 이렇게 구성
* 하나의 코어에 해당하는 것은 SM 하나에 들어있는 스트리밍 프로세서(Streaming Processor, SP)
  * SP는 보다시피 SM, TPC와 같은 계층에 속해있어 연산이나 데이터 접근에 다소 제약과 차이가 있다.
* 이 구조는 총 128개의 스트리밍 프로세서가 있다.

스트리밍 프로세서의 구조
* 레지스터 파일과 3개의 ALU
* ALU는 각각 부동소수점, 정수 연산, 비교와 데이터 이동을 처리

지금까지 설명한 구조를 다시 정리

![11-05](https://github.com/MyungHyun-Ahn/SystemProgramming/assets/78206106/3ec45340-bde9-4fd6-b678-bd5e5443f8f9)
* 가장 기본적인 코어 : 스트리밍 프로세서
* 8개가 모여 : 하나의 스트리밍 멀티프로세서
* L1 캐시와 순차적으로 명령어를 인출하는 장치가 있다.
* 8개의 SP가 공유하는 작은 메모리도 있다.
* SFU(Special Function Unit) : 제곱근이나 삼각함수 등 복잡한 연산을 위한 장치 두 개 존재
* SM은 이제 두 개씩 쌍을 이뤄 텍스처 공유 메모리와 함께 TPC를 형성
* TPC가 배열로 나열되면 이것이 즉, GPU

## CUDA 프로그래밍 모델 : 스레드와 메모리 모델
CUDA 프로그래밍 모델
* 헤테로지니어스 멀티코어를 위한 프로그래밍 방법론 중 하나
* C언어에 기반한 프로그래밍 언어로 CUDA 전용 컴파일러를 이용해 GPU에서 돌아갈 프로그램을 컴파일하고 나머지 CPU에서 작동하는 코드를 컴파일하여 합쳐서 실행

프로그래밍 방법론 중 결정해야할 중요한 문제 - 메모리 모델
* 싱글스레드 CPU를 위한 메모리 모델은 가상 메모리라는 개념
* 멀티코어로 가면 각 코어간에 주소 공간이 공유 여부에 따라 프로그래밍 방법론이 달라진다.
  * 병렬 프로그래밍 방법론 (Story 18)
  * 이곳이 어렵다면 Story 18을 본 뒤 다시 보는 것 추천

CUDA 프로그래밍은 CPU와 GPU가 주소 공간을 공유하지 않음
* 서로 명시적으로 데이터를 주고받아야 작업이 가능
* 각자 메모리가 있으므로 이런 접근은 자연스럽다.
  * 인텔이 제시하는 멀티코어와 그래픽코어의 방법론은 두 다른 코어가 공유 메모리 공간을 허용
* 명시적으로 GPU가 수행할 코드를 CPU가 내려보내야 한다.

CUDA 프로그래밍의 실행 흐름

![11-06](https://github.com/MyungHyun-Ahn/SystemProgramming/assets/78206106/465d34e3-4c1f-4179-94c5-fbb05145cce9)
* 명시적으로 처리할 데이터를 GPU 메모리로 전송
* GPU는 이걸 실행하고 CPU는 최종 결과를 다시 받아온다.

CUDA 프로그래밍의 스레딩 모델
* CPU의 경우 코어 당 기본적으로 하나, SMT라면 2개 혹은 4개지만 그 수가 제한적
* GPU는 풍부한 데이터 병렬성을 효과적으로 처리해야 하므로 아주 많은 스레드의 문맥을 관리

CUDA 프로그래밍 모델은 SPMD(Single Program Multiple Data)의 형태로 작동
* 하나의 코드가 여러 데이터를 동시에 처리하는 구조 SIMD와 개념은 같다.
* GPU 스레드, nVidia 구조의 스레드는 미세 단위인 데이터 수준으로 작동
* 이러한 접근 방식으로 CUDA는 스레드 계층을 두어 관리

큰 단위부터
* 그리드(grid) 스레드
* 블록(block) 스레드

### CUDA의 스레드 계층 구조

![11-07](https://github.com/MyungHyun-Ahn/SystemProgramming/assets/78206106/24c9638c-729d-4783-8ae5-de7fcf10c9ba)
* CPU가 GPU에 실행할 코드를 내려보내는데, 이 코드를 커널(kernel)이라고 한다.
* 커널이 GPU에 전송될 때 어떤 스레드 계층 구조로 실행될지 정한다.
* 커널은 같은 코드로 다른 데이터를 처리한다.
* 한 커널은 하나의 그리드에서 실행, 그리드는 2차원의 블록으로 구성
* 이 코드는 Grid 1dl 2x2, 4개의 블록을 가진 구조를 보여준다.
* 블록은 다시 3차원의 스레드 구조로 세분화
* 그림에서 블록은 2x2, 스레드는 블록마다 16개
* 그리드에는 16 * 4 = 64개의 스레드가 있다.
* 이렇게 2차원, 3차원의 형태를 허용하는 것은 GPU가 데이터 병렬성을 쉽게 활용하기 위함

3차원 공간에 있는 유체에 수치적으로 편미분 방정식을 푼다면
* 데이터 병렬성의 전형적인 접근 방식은
* 큰 유체의 볼륨을 작은 사각형으로 나누는 것
* 그림의 3차원 스레드 구조를 그대로 대입하면 된다.

정리
* 스레드는 어떤 블록의 소속
* 블록은 어떤 그리드의 소속

이를 구별할 수 있는 식별자가 반드시 필요
* 같은 코드가 한 그리드 내의 모든 스레드에서 작동되므로 필수적
* 이런 식별자를 CUDA 프로그래밍은 아예 자체 변수로 제공

이런 변수는 모두 dim3라는 기본 정의된 struct 형태로 x, y, z 멤버 변수가 있다.
* gridDim : 현재 실행하는 커널이 있는 그리드 차원(최고 2차원)
* blockDim : 현재 커널의 블록 차원(최고 3차원)
* blockIdx : 현재 그리드 내에서 자신의 블록 아이디
* threadIdx : 현재 블록 내에서 자신의 스레드 아이디

그림을 예로
* gridDim(2, 2) 면 blockDim(4,2,2)
* threadIdx(1,0,0)이면 blockIdx(1,1)
* 이러한 방식으로 손쉽게 내가 어떤 데이터를 처리해야 하는지 계산 가능

### CUDA 디바이스 메모리 모델

![11-08](https://github.com/MyungHyun-Ahn/SystemProgramming/assets/78206106/2fc01d36-5133-4dcf-9e0e-fb96d25b5b57)
* GPU는 그래픽에 특화, 데이터 병렬성이 풍부, 시간적 지역성(temporal locality)가 그리 높지 않은 스트리밍 응용프로그램에 최적회
* 따라서 CPU 처럼 큰 캐시도 없고 L1 캐시도 캐시 코히런시가 지켜지지 않는다.

그림을 정리하면
1. 디바이스는 스레드마다 레지스터에 읽고 쓸 수 있다.
2. 디바이스는 스레드마다 로컬 메모리에 읽고 쓸 수 있다.
3. 디바이스는 블록마다 공유 메모리에 읽고 쓸 수 있다.
4. 디바이스는 그리드마다 글로벌 메모리에 읽고 쓸 수 있다.
5. 디바이스는 그리드마다 constant 메모리에 쓰고 읽을 수만 있다.
6. 호스트는 그리드마다 글로벌과 컨스턴트 메모리에 읽고 쓸 수 있다.

쉽게 설명하면
* 현재 커널이 수행하는 모든 스레드가 모두 같은 메모리를 보는 것이 아니다.
* 1, 2는 자명하다. 2는 TLS(Thread-Local Storage)처럼 생각해도 된다.
* 3을 보면 공유 메모리 공간은 같은 블록 내에서만 서로 볼 수 있다.
* 공유하려면 명시적으로 글로벌 메모리를 거쳐야 한다.

같은 블록 내의 스레드는 공유 메모리로 더 긴밀히 작업을 함께 할 수 있다.
* 같은 블록 내의 스레드만이 스레드 동기화의 대상이 된다.
* 즉, 다른 블록은 항상 독립적으로 돌아간다.

## CUDA 프로그래밍의 예 : 행렬 곱셈

## nVidia GPU의 자세한 스레드 실행 구조 : 워프(Warp)
nVidia의 스레드 스케줄링은 CPU와 운영체제의 전통적인 스레드 스케줄링과는 상당히 다르다.
* 하드웨어 멀티스레딩에서의 스케줄링 문제로 봐야한다.
  * 미세단위/큰단위 멀티스레딩, 동시 멀티스레딩
* 마찬가지로 GPU에도 이런 스레드 스케줄링이 있다.
  * CUDA 프로그래밍 모델은 이러한 스케줄링을 정의하지 않는다.
  * GPU의 구현 정책에 달림

GPU의 스레드 스케줄링을 설명하는데 워프라는 개념을 이용
* 32개의 스레드는 하나의 워프를 이루며, 이것이 GPU 스레드 스케줄링의 단위
* 하나의 블록은 최대 512개의 스레드를 가질 수 있고 이 블록이 SM에 할당
* 스케줄링 단위로 블록 내 스레드를 32개씩 묶어 워프라고 정의
  * 이것은 구현에 달린 값으로 더 커질 수도 있다.
  * 블록 512개는 CUDA 프로그래밍에서 정의하는 것
  * CUDA에는 워프라는 개념은 없다.

예를 들어, 세 블록이 하나의 SM에 할당, 한 블록이 256개의 스레드를 가지고 있다고 하자.
* 256 / 32 = 8개의 워프가 한 블록 내에 있고
* 3개의 블록이 SM이 있으므로 24개의 워프가 SM에 할당된 것
* 24개의 워프, 768개의 스레드 8800GTX GPU가 처리할 수 있는 최대 워프/스레드 값

최대 24개의 워프가 하나의 SM에 머무를 수 있다.
* 머무른다 : 최대 24개 워프, 768개 스레드가 문맥을 하나의 SM에서 유지할 수 있다는 것
* 문맥 : 스레드 하나가 사용하는 레지스터
* 8800GTX에서 하나의 SM에 8192개 레지스터가 있다.
* 스레드 하나가 10개의 레지스터를 쓰고 블록 하나가 256개의 스레드로 구성된다면 하나의 블록은 2560개의 레지스터를 쓴다.
* 따라서 최대 3개의 블록을 SM에 할당 가능
* 그러나 스레드 하나가 12개의 레지스터만 써도 12 * 256 = 3072개의 레지스터가 블록마다 필요
  * 8800GTX에서 SM는 2개의 블록만 작동시킬 수 있다.

워프는 SM에 최대 24개까지 있을 수 있지만 오직 하나만 실행
* 한 워프는 처리하는 코드가 같으므로 SIMD 형태로 작동
* 워프는 32개의 스레드 8800GTX는 8개의 SP를 가지므로
* 워프 하나에 있는 스레드 명령어를 모두 시작하려면 4사이클 걸린다.
* 다행이 ALU가 모두 파이프라인화 되어 있어 사이클마다 이상적으로 워프에서 명령어를 읽어올 수 있다.
  * 물론 분기문이나 다른 장애물이 있으면 이야기는 달라진다.

SM이 한 번에 하나밖에 워프를 처리하지 못하는데, 왜 24개까지 있을 수 있게 할까?
* 하드웨어 멀티스레딩이 유용했던 이유는 레이턴시가 긴 명령어를 처리할 때 놀고 있는 자원이 다른 스레드의 명령어를 처리하기 때문
* 마찬가지로 어떤 워프가 메모리 연산이 오래걸릴 때 SM에 있는 워프 스케줄러가 다른 워프의 명령어를 읽어 투입
* GPU는 메모리에서 데이터를 많이 가져오고 캐시의 양이 적기 때문에 메모리 연산이 빈번
* 따라서 하드웨어 멀티스레딩은 필수적
* 스케줄링 정책은 그래픽 카드 구현에 달렸다.