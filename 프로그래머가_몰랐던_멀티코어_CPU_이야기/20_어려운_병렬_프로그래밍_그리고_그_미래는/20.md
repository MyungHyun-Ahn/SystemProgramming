# Story 20 어려운 병렬 프로그래밍, 그리고 그 미래는?

## 비효율적인 병렬 프로그래밍 : 가짜 공유 문제
가짜 공유 문제(false sharing)
* 대표적인인 비효율적인 병렬 프로그래밍 원인 중 하나
* 실제로는 공유되지 않는 데이터인데 캐시 구조의 특성으로 마치 공유되는 것으로 인식되어 불필요한 성능 저하 현상이 일어나는 것을 가리킨다.

~~~C++
#include <windows.h>
#include <stdio.h>
#include <tchar.h>

volatile int data1;
volatile int data2;

DWORD CALLBACK TestThread1(void /*arg*/)
{
    // 0번 CPU에만 작동
    SetThreadAffinityMask(GetCurrentThread(), 1 << 0);
    for (int i = 0; i < 1500000000; ++i)
        data1 = data1 + 1;
    return data1;
}

DWORD CALLBACK TestThread2(void /*arg*/)
{
    // 2번 CPU에만 작동
    SetThreadAffinityMask(GetCurrentThread(), 1 << 2);
    for (int i = 0; i < 1500000000; ++i)
        data2 = data2 + 1;
    return data2;
}

int _tmain(int argc, _TCHAR *argv[])
{
    HANDLE thread[2];
    SetPriorityClass(GetCurrentProcess(), HIGH_PRIORITY_CLASS);

    DWORD startTime = GetTickCount();
    thread[0] = CreateThread(NULL, 0, TestThread1, 0, 0, 0, 0);
    thread[1] = CreateThread(NULL, 0, TestThread2, 0, 0, 0, 0);
    WaitForMultipleObjects(2, thread, TRUE, INFINITE);
    _tprintf(_T("%d\n"), GetTickCount() - startTime);
    return 0;
}
~~~
* Win32 API를 이용해 만든 간단한 멀티스레드 프로그램
* volatile로 선언된 서로 다른 두 정수를 각각 15억번 씩 더한다.
  * volatile로 선언하지 않는다면 컴파일러가 최적화를 통해 미리 덧셈한 최종 결과만 써버린다.
* 프로그램은 정확하게 돌아가지만 멀티코어에서는 성능 상의 문제가 발생할 수 있다.
  * 가짜 공유 문제

### 가짜 공유 문제

![19-05](https://github.com/MyungHyun-Ahn/SystemProgramming/assets/78206106/5d6042a8-6790-46af-903d-76163da63023)
* 같은 캐시 라인에 공유되지 않는 데이터가 같이 놓일 때 발생한다.
* 각 코어는 각자 L1 캐시(private cache)를 가진다.
  * 읽고 쓰는 단위가 64바이트와 같은 캐시 라인 크기
* data1과 data2가 연이어 선언되어 같은 캐시 라인에 놓인다.

문제는 캐시 코히런스에서 발생한다.
* 멀티코어에서는 캐시는 사본이 있더라도 모든 코어가 항상 최신 내용을 볼 수 있도록 처리한다.
* 캐시 코히런시 프로토콜을 떠올리자
* 어떤 코어가 한 캐시 라인을 쓰려면 다른 코어가 가진 사본을 모두 무효화시키는 신호를 보내야만 했다.
  * 이것이 바로 가짜 공유의 근본 원인
* data1을 갱신할 때 data2가 있는 캐시라인 또한 무효화가 되어 버린다.
* 실제로는 데이터가 공유되지 않지만 서로 끊임없이 서로의 캐시 라인을 무효화하고 그에 따라 캐시미스가 계속 발생하고 만다.


가짜 공유 문제는 경우에 따라 심각한 성능 문제
* L1, L2가 모두 전용 캐시라면 상대방의 L1, L2 캐시 라인을 모두 없애고
* 그 결과 캐시 라인을 잃은 캐시는 최악의 경우 메모리에서 데이터를 가져와야 한다.
* 최근의 멀티코어는 마지막 레벨 캐시가 공유되므로 피해가 상대적으로 적다.
* 과거 멀티프로세서 환경에서 가짜 공유가 일어났을 때도 문제가 심각했다.
  * 특히 베리어를 구현할 때 가짜 공유 문제를 회피하는 것이 중요한 설계 조건 중 하나였다.

위 소스의 문제점을 고쳐보자
~~~C++
#define CACHE_LINE 64 // 프로세서 종류마다 다르다. 대부분 64
#define CACHE_ALIGN_MSVC \
  __declspec(align(CACHE_LINE)) // VC++
#define CACHE_ALIGN_GCC \
  __attribute__((aligned)) // GCC

volatile CACHE_ALIGN_MSVC int data1;
volatile CACHE_ALIGN_MSVC int data2;

volatile struct {
  int data1;
  char padding[CACHE_LINE];
  int data2;
} DATA;
~~~
* data1과 data2를 다른 캐시 라인에 두면 된다.
  * 컴파일러가 지원하는 기능 사용
  * 구조체를 만들어 패딩을 채우는 방법

실제 가짜 공유로 인한 성능 손실은 얼마나 될까?
* 인텔 Core i7 920 CPU(쿼드코어 + 하이퍼스레딩, L3 공유 캐시)에서 컴파일러 최적화 후 속도를 비교
* 가짜 공유 여부로 25% 정도의 성능 차이
* 과거 Pentium D 같은 공유 캐시가 없는 구조는 200% 이상 차이도 날 수 있다고 한다.

가짜 공유 문제는 new, malloc 과 같은 동적 할당에서도 일어날 수 있다.
* 기본 힙 할당자는 멀티스레드 환경에 최적화되지 않음
  * 연속해서 할당하면 같은 캐시 라인에 놓일 확률이 있다.
* 따라서 멀티스레드 환경에서는 이를 고려한 힙 할당자를 쓰는 것이 좋다.
* 추가로 힙 할당자는 락으로 내부 자료구조를 보호하기 때문에 병목 지점이 될 수 있다.
* 이런 것들을 해결한 할당자
  * 스레드 빌딩 블록(TBB)의 tbbmalloc과 HOARD 할당자

도구의 부족도 비효율적인 병렬 프로그래밍의 큰 원인
* 아직 쓸만한 도구가 없다.
* 예를들어 병렬화 하였을 때 얼마나 스피드업 되었는지 예측하는 도구 등등,,,

## 미래의 병렬 프로그래밍 방법론
새로운 하드웨어의 도입을 전제로 더 쉬운 병렬 프로그래밍 방법론이 제안됨
* 트랜잭셔널 메모리
* 스레드 수준 투기 방법

병렬 프로그래밍에서 가장 어려운 것
* 공유 데이터 관리의 어려움
  * 동기화 객체(락)로 배타적 접근을 보장해야만 한다.
  * 그런데 락은 문제가 많다.
    * 빼먹는 경우.. 등
    * 우선 순위 역전현상, 락 콘보잉, 데드락이 대표적인 예
    * 또한 성능에도 좋지 않다.

스택을 구현할 때
* 간단하게 함수의 시작과 끝을 락으로 보호
* 정확하게는 돌아가겠지만, 병행성이 낮고 성능에 대한 확장성이 떨어진다.

락 단위에 따른 구분
* coarse-grained locking : 큰 단위로 거는 것
* fine-grained locking : 미세한 단위로 거는 것
  * 병행성은 높일 수 있다.
  * 프로그램 작성이 어려워지고 버그가 발생할 확률도 매우 커진다.

락 합성은 매우 어렵다. 큐의 예
~~~C++
void TransferValue(FIFO Q1, FIFO Q2)
{
  Q1.lock.acquire();
  Q2.lock.acquire();
  v = Q1.dequeue();
  Q2.enqueue(v);
  Q2.lock.release();
  Q1.lock.release();
}
~~~
* FIFO 큐의 한 원소를 읽어 다른 큐로 옮기는 작업
* 그런데 데드락이 발생할 확률이 있다.
* TransferValue(Q1, Q2)와 TransferValue(Q2, Q1)을 동시에 호출하면 데드락에 빠진다.
* 이렇게 dequeue, enqueue 연산을 원자적으로 만들어도 두 연산의 합을 원자적으로 하는 것은 간단하지 않다.

이것을 해결하기 위한 TM(Transactional Memory)
~~~C++
void TransferValue(FIFO Q1, FIFO Q2)
{
  __atomic { // 트랜잭션 영역 시작
    v = Q1.dequeue();
    Q2.enqueue(v);
  } // 트랜잭션 끝
}
~~~
* 이 코드 영역은 원자적으로 실행되어라 라고 표기하는 방법
* 그냥 트랜잭션 영역만 표기하면 이 코드가 안전하게 실행됨을 보장한다.
* 반면 락은 항상 최악의 상황을 가정하여 자신만이 이 데이터를 접근함에도 항상 락을 잡는 작업을 해야한다.
* 트랜잭션이 충돌을 일으키지 않는다면 락을 잡지 않으므로 성능이 향상
* 그러나 만약 충돌이 감지되면 시스템은 이 트랜잭션을 무효화하고 Undo 한다.
* 주의할 것은 TM 영역 안에 외부 입출력이 있다면 작업을 취소하고 재실행하기 매우 어려울 것

TM의 아이디어의 기원 데이터베이스의 트랜잭션
* DB의 트랜잭션은 ACID(Atomicity, Consistency, Isolation, Durability)라는 조건을 만족하는 논리적 실행 단계
* TM은 ACID를 모두 따르지 않고 A와 I를 보장한다.

TM은 락 프리 자료구조를 일반적으로 구현하기 위한 하드웨어 지원 방법에서 제안되었다.
* 락프리 자료구조는 뮤텍스를 쓰지 않고 cas 원자적 연산을 직접적으로 이용하여 스핀락으로 구현한다.
* 일반적인 뮤텍스는 획득 경쟁에 실패하면 스케줄링에 빠져버린다.
  * 대신 CPU 자원은 사용하지 않는다.
* 락 프리 자료구조는 스핀 락을 돌지만 블록킹 되는 일이 없다.
  * 뮤텍스 기반 자료구조보다 더 높은 병행성을 얻을 수 있다.
  * 물론 경쟁이 심하다면 락프리 자료구조는 CPU 자원을 과도하게 낭비할 수 있다.
* 락 프리 자료구조는 하드웨어의 제약으로 일반적인 자료구조 형태로 확장하기 어렵다.
  * 리스트, 큐, 스택마다 각각 특화해서 만듬


TM은 많은 락의 문제점을 해결해준다.
* 원자성 위반, 데드락, 우선순위 역전 등,,
* 그러나 순서 위반의 경우에는 도움이 되지 않는다.
  * TM의 구조와 문법이 A가 반드시 B보다 앞서 수행된다는 것을 기술하기 어렵기 때문

간략한 TM의 하드웨어적 구현
* 핵심 구현 기능 중 하나는 데이터 충돌 감지
  * 캐시 코히런스가 해결
  * 즉, 가장 간단한 해결법은 캐시 코히런스를 확장하는 것
  * 그러나 트랜잭션의 크기가 캐시보다 커지면 처리가 곤란하다.
* 위 문제를 극복하고 모든 규모의 트랜잭션을 처리하는 HTM을 Unbounded HTM이라 함
* 아직까지도 TM을 어떻게 지원해야할지는 논의가 활발하다.

TM과 비슷한 것 스레드 수준 투기(Thread-level Speculation, TLS)
* 투기적 실행을 스레드 수준까지 끌어올리는 아이디어
* 어떤 코드가 안전하게 병렬로 실행될지 모르는 상황에서 강제로 여러 스레드로 나뉘어 실행하고 본다.
* 하드웨어는 끊임없이 데이터가 충돌되는지 살펴본다.
  * 충돌이 없다면 성능 향상
  * 만약 충돌이 있다면 rollback하고 재실행
* TLS는 프로그래머가 넣는다가보다 컴파일러가 적절하게 넣는다.
* 아직 실제 하드웨어는 구현되지 않아 이론상으로만 존재한다.

